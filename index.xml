<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Somtom&#39;s Blog on Somtom&#39;s Blog</title>
    <link>/</link>
    <description>Recent content in Somtom&#39;s Blog on Somtom&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Dynamically Rendered Flexdashboard Pages Using RMarkdown Childs</title>
      <link>/post/using-dynamically-rendered-r-markdown-childs-for-reports/</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/using-dynamically-rendered-r-markdown-childs-for-reports/</guid>
      <description>&lt;p&gt;Dashboards are nice tools when it comes to analyzing quickly changing data. Nevertheless, you may also use them for reporting purposes by taking snapshots. R has a nice library called &lt;code&gt;flexdashboard&lt;/code&gt; which you can use for creating dashboards out of &lt;code&gt;RMarkdown&lt;/code&gt; files. During a project at work we found a nice solution for dynamically rendering flexdashboard pages by using R Markdown child templates which I want to share with you in this blog post.&lt;/p&gt;
&lt;p&gt;In our project we wanted to dynamically create a dashboard-style report. It should automatically fetch data by the use of parameters which are specified and passed to the knitting function. This can easily be solved by using &lt;strong&gt;RMarkdown parameters&lt;/strong&gt;. An additional problem we had though, was that we wanted the report to have an &lt;strong&gt;overview page&lt;/strong&gt; with important information but also &lt;strong&gt;subpages&lt;/strong&gt; with dynamically knitted detail analysis. The subpages should all have the same format but their number would be depending on the fetched data (i.e. number of different subgroups).&lt;/p&gt;
&lt;div id=&#34;example-use-case&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example Use Case&lt;/h1&gt;
&lt;p&gt;Let’s go through the problem and our approach with a made up example:&lt;br /&gt;
Imagine we want to analyse the performance of a company’s products. We plan to have an overview page where we show summarized information about all products of the company. This could for example be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Trade volume by time&lt;/li&gt;
&lt;li&gt;Product groups’ share on company’s trade volume and order count&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The recipient of the report also wants a deeper analysis of each product group in order to gain more detailed information. Therefore, we plan to create subpages within our report which show information like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Trade volumes share of products within their product group&lt;/li&gt;
&lt;li&gt;Products with most trade volume&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following gif shows how the report could look like. It is probably not the most sophisticated and beautiful report you have ever seen but it should be OK for example purposes:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2019-01-19-using-dynamically-rendered-r-markdown-childs-for-reports_files/example.gif&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;While each subpage should have the exact same format, the count of pages will depend on the data (i.e. the product groups) we fetch for a specific company. The approach we show in this post is to use a &lt;code&gt;overview.RMD&lt;/code&gt; file which has all the code for the &lt;strong&gt;overview page&lt;/strong&gt; and a &lt;code&gt;subpage.RMD&lt;/code&gt; file which should serve as a template for the &lt;strong&gt;subpages&lt;/strong&gt;. We then want to somehow render the subpage for each product group in the data (i.e. by using a &lt;em&gt;for-loop&lt;/em&gt; within &lt;code&gt;overview.RMD&lt;/code&gt;) and integrate all these pages into one report. The following diagram should give you an abstract idea of the process we plan to implement:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2019-01-19-using-dynamically-rendered-r-markdown-childs-for-reports_files/main_subpage.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-do-it&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Let’s do it!&lt;/h1&gt;
&lt;p&gt;We will go through our approach step by step and I am going to show you the important code chunks you need. The following list will give you an overview on the issues we need to master in order to integrate the subpage template into &lt;code&gt;overview.RMD&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dynamically create page name for each product group and add it to a navbar menu&lt;/li&gt;
&lt;li&gt;Dynamically create page content for given product group &lt;code&gt;Product Groups&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Combine knitted content of all subpages and add it to bottom of &lt;code&gt;overview.RMD&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;preparations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preparations&lt;/h2&gt;
&lt;p&gt;Before we tackle those issues, we need to find all unique product groups we want to create the subpages for, and initialize a &lt;code&gt;out&lt;/code&gt; variable which stores all content knitted within our loop. Additionally, we need to change the &lt;code&gt;knitr&lt;/code&gt; options to allow duplicated chunk labels. Otherwise we cannot loop through &lt;code&gt;subpage.RMD&lt;/code&gt; anymore if we would use the same chunk labels.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get all unique product groups for the subpages
product_groups &amp;lt;- unique(data$product_group)

# Create variable which stores all subpages outputs
out = NULL

# Set knitr options to allow duplicate labels (needed for the subpages)
options(knitr.duplicate.label = &amp;#39;allow&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first trick we use for our subpage rendering is the creation of a &lt;strong&gt;separate environment&lt;/strong&gt; which stores the data needed in &lt;code&gt;subpage.RMD&lt;/code&gt; (i.e. the filtered data). We create this environment and call it &lt;code&gt;subpage_env&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create temporary environment which we use for knitting subpage.RMD 
subpage_env &amp;lt;- new.env()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dynamic-page-name-in-navbar-menu&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dynamic Page Name in Navbar Menu&lt;/h2&gt;
&lt;p&gt;To dynamically create a navbar entry for a product group page during the knitting process, we use some code within &lt;code&gt;subpage.RMD&lt;/code&gt; which does the job:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;`r paste0(&amp;#39;# &amp;#39;, product_group, &amp;#39;{data-navmenu=&amp;quot;Product Groups&amp;quot;}&amp;#39;)`&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This line will create a level 1 header which &lt;code&gt;flexdashboard&lt;/code&gt; translates into a new page. As we use &lt;code&gt;{data-navmenu=&amp;quot;Product Groups&amp;quot;}&lt;/code&gt;, all product group pages will be placed within a navbar menu called &lt;code&gt;Product Groups&lt;/code&gt;. The entry will be the product group’s name. Basically this creates a markdown line which could for example look like this: &lt;code&gt;# PG1 {data-navmenu=&amp;quot;Product Groups&amp;quot;}&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;filter-data-and-knit-subpage&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Filter Data and Knit Subpage&lt;/h2&gt;
&lt;p&gt;Now everything is prepared and we can loop through each product group, filter the data accordingly, assign the corresponding variables to &lt;code&gt;subpage_env&lt;/code&gt; and knit the page using our subpage environment. The knitted result will be added to our &lt;code&gt;out&lt;/code&gt; variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (pg in product_groups) {
  # Filter data for product group 
  subpage_data &amp;lt;- data %&amp;gt;% 
    filter(product_group == pg)
  
  # Assign filtered data and product group to subpage_env 
  assign(&amp;quot;subpage_data&amp;quot;, subpage_data, subpage_env)
  assign(&amp;quot;product_group&amp;quot;, pg, subpage_env)
  
  # Knit subpage.RMD using the subpage_env and add result to out vector
  out = c(out, knitr::knit_child(&amp;#39;subpage.RMD&amp;#39;, envir = subpage_env))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is basically it. Let me know what you think of our approach or if you have any improvement ideas on it. You might also want to have a closer look into the example files &lt;code&gt;overview.RMD&lt;/code&gt; and &lt;code&gt;subpage.RMD&lt;/code&gt; which you find at the bottom of this post.&lt;/p&gt;
&lt;p&gt;Find a demo of the dashboard we created in this post &lt;a href=&#34;/post/2019-01-19-using-dynamically-rendered-r-markdown-childs-for-reports_files/dashboard.html&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;further-reading&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further Reading&lt;/h1&gt;
&lt;p&gt;I suggest going to the documentation of &lt;code&gt;flexdashboard&lt;/code&gt; if you want to learn more about the package. Furthermore, I want to provide you with some additional links which helped me evolving the idea.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/&#34;&gt;Flexdashboard documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/21729415/generate-dynamic-r-markdown-blocks/21730473#comment65652301_21730473&#34;&gt;Stackoverflow question&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;markdown-files&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Markdown Files&lt;/h1&gt;
&lt;div id=&#34;overview.rmd&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview.RMD&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;---
title: &amp;quot;Example Analytics&amp;quot;
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: fill
---
# Overview

`r shiny::tags$h1(&amp;quot;Overview&amp;quot;, style=&amp;quot;text-align:center&amp;quot;)`

```{r setup, include=FALSE}
library(flexdashboard) 
library(tidyverse)

# Create dummy data for example
set.seed(345)
date_start &amp;lt;- as.Date(&amp;quot;2018-01-01&amp;quot;)
product_groups &amp;lt;- c(&amp;quot;PG1&amp;quot;, &amp;quot;PG2&amp;quot;, &amp;quot;PG3&amp;quot;)
products &amp;lt;- list(
  PG1 = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;),
  PG2 = c(&amp;quot;D&amp;quot;, &amp;quot;E&amp;quot;, &amp;quot;F&amp;quot;),
  PG3 = c(&amp;quot;G&amp;quot;, &amp;quot;H&amp;quot;)
)
data &amp;lt;- data.frame(
  product_group = sample(product_groups, 100, replace = T),
  trade_volume = rnorm(100, mean = 3448, sd = 200),
  date = date_start + 1:100
) %&amp;gt;% 
  group_by(id = row_number()) %&amp;gt;% 
  mutate(product = sample(unlist(products[[product_group]]), 1, replace = TRUE)) %&amp;gt;% 
  ungroup()
```


Row
-----------------------------------------------------------------------

### Trade Volume Development

```{r fig.width=15, fig.height=5}
data %&amp;gt;% 
  group_by(date) %&amp;gt;% 
  summarise(trade_volume = sum(trade_volume)) %&amp;gt;% 
  ggplot() +
  geom_line(aes(x = date, y = trade_volume), 
           color = &amp;quot;#2780e3&amp;quot;,
           alpha = 0.8) +
  scale_y_continuous(labels = scales::dollar) +
  theme_minimal() +
  ylab(&amp;quot;Trade Volume&amp;quot;) +
  xlab(&amp;quot;Date&amp;quot;)
```


Row
-----------------------------------------------------------------------

### Product Group Share on Trade Volume

```{r}
data %&amp;gt;% 
  group_by(product_group) %&amp;gt;% 
  summarise(trade_volume = sum(trade_volume)) %&amp;gt;%
  mutate(trade_volume_share = trade_volume / sum(trade_volume)) %&amp;gt;% 
  ggplot() +
  geom_col(aes(x = reorder(product_group, -trade_volume_share), y = trade_volume_share), 
           fill = &amp;quot;#2780e3&amp;quot;,
           alpha = 0.8) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal() +
  ylab(&amp;quot;Share on Trade Volume&amp;quot;) +
  xlab(&amp;quot;Product Group&amp;quot;)
```

### Product Group Share on Order Count

```{r}
data %&amp;gt;% 
  group_by(product_group) %&amp;gt;% 
  summarise(order_count = n()) %&amp;gt;% 
  mutate(order_count_share = order_count / sum(order_count)) %&amp;gt;% 
  ggplot() +
  geom_col(aes(x = reorder(product_group, -order_count_share), y = order_count_share), 
           fill = &amp;quot;#2780e3&amp;quot;,
           alpha = 0.8) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal() +
  ylab(&amp;quot;Share on Order Count&amp;quot;) +
  xlab(&amp;quot;Product Group&amp;quot;)
```

```{r render subpages, include=FALSE}
# Get all unique product groups for the subpages
product_groups &amp;lt;- unique(data$product_group)

# Create variable which stores all subpages outputs
out = NULL

# Set knitr options to allow duplicate labels (needed for the subpages)
options(knitr.duplicate.label = &amp;#39;allow&amp;#39;)

# Create temporary environment which we use for knitting subpages.RMD 
subpage_env &amp;lt;- new.env()

for (pg in product_groups) {
  # Filter data for product group 
  subpage_data &amp;lt;- data %&amp;gt;% 
    filter(product_group == pg)
  
  # Assign filtered data and product group to subpage_env 
  assign(&amp;quot;subpage_data&amp;quot;, subpage_data, subpage_env)
  assign(&amp;quot;product_group&amp;quot;, pg, subpage_env)
  
  # Knit subpage.RMD using the subpage_env and add result to out vector
  out = c(out, knitr::knit_child(&amp;#39;subpage.RMD&amp;#39;, envir = subpage_env))
}
```

`r paste(knitr::knit_child(text = out), collapse = &amp;#39;&amp;#39;)`&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;subpage.rmd&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Subpage.RMD&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;`r paste0(&amp;#39;#&amp;#39;, product_group, &amp;#39;{data-navmenu=&amp;quot;Product Groups&amp;quot;}&amp;#39;)`

`r shiny::tags$h1(product_group, style=&amp;quot;text-align:center&amp;quot;)`

Row
-----------------------------------------------------------------------

### Product Share on Trade Volume

```{r fig.width=10}
subpage_data %&amp;gt;% 
  group_by(product) %&amp;gt;% 
  summarise(trade_volume = sum(trade_volume)) %&amp;gt;% 
  ggplot() +
  geom_col(aes(x = reorder(product, -trade_volume), y = trade_volume), 
           fill = &amp;quot;#2780e3&amp;quot;,
           alpha = 0.8) +
  scale_y_continuous(labels = scales::dollar) +
  theme_minimal() +
  ylab(&amp;quot;Trade Volume&amp;quot;) +
  xlab(&amp;quot;Product&amp;quot;)
```


### Products with Most Trade Volume

```{r}
subpage_data %&amp;gt;% 
  group_by(Product = product) %&amp;gt;% 
  summarise(`Trade Volume [â‚¬]` = sum(trade_volume)) %&amp;gt;% 
  arrange(-`Trade Volume [â‚¬]`) %&amp;gt;% 
  knitr::kable()
```&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Munich Bike Data Info Graphic</title>
      <link>/post/munich-bike-data-info-graphic/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/munich-bike-data-info-graphic/</guid>
      <description>&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/rmarkdown-libs/infographic/insights_on_bike_traffic.png&#34; /&gt;

&lt;/div&gt;
&lt;div id=&#34;analysis&#34; style=&#34;display: none&#34;&gt;
&lt;hr /&gt;
&lt;p&gt;During this year’s summer break there had been a railway construction site which affected the train I usually use for commuting to work. This led to a lot of additional commuting time. Thus, I decided to repair my old mountain bike and give it a try to go to work by bike.&lt;br /&gt;
During the one hour ride I wondered how many people made the same decision and if there would be any data about Munich bike traffic with the use of which I could get some insights about Munich’s bike traffic. Shortly after that, I happened to see a twitter post of &lt;a href=&#34;https://twitter.com/opendata_muc/status/1039758780853104646&#34;&gt;OpenData Munich&lt;/a&gt; which announced a new data set containing the data of Munich’s 6 continuous bike counting stations for the years 2017 and 2018. The counting stations provide data such as 15-minute and daily bike counts for 2 directions.&lt;/p&gt;
&lt;p&gt;I contacted the team in order to ask for additional information and received even more data as well as some nice inspirations. I picked up the idea of creating a info-graphic about Munich bike data. Thus, I had a new project and started the journey of designing my first info-graphic.&lt;/p&gt;
&lt;p&gt;In this post we will go through the analysis behind the info-graphic you already saw above. We are going to answer questions such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#station_map&#34;&gt;Can we create a map of all counting stations?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#average_counts&#34;&gt;What are average 15 minute and daily counts (across stations and per station)?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wrong_direction&#34;&gt;How many people are driving in the wrong direction at Arnulf Street (this station is at a one-way road)?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#count_records&#34;&gt;What are the highest counts which had been measured?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bike_times&#34;&gt;When are most people biking (time of the year, weather, …)?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#commuting&#34;&gt;How does the commuting traffic look like? Can we model it somehow and get some insights out of the data?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want to skip the data import and jump directly to the preprocessing step, click &lt;a href=&#34;#preprocessing&#34;&gt;here&lt;/a&gt;. To directly get into the analysis, click &lt;a href=&#34;#analysis&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;set-up-environment&#34;&gt;Set up Environment&lt;/h1&gt;
&lt;p&gt;First, as always, we load some packages needed for this analysis and create a custom theme to make the plots look nicer.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages({
  library(tidyverse)
  library(lubridate)
  library(geosphere)
  library(ggmap)
  library(reshape2)
  library(modelr)
  library(broom)
})

background_colors &amp;lt;- c(&amp;quot;#efedec&amp;quot;, &amp;quot;white&amp;quot;)
plot_colors &amp;lt;- c(&amp;quot;#62c3db&amp;quot;, &amp;quot;#136d8f&amp;quot;, &amp;quot;#072d40&amp;quot;, &amp;quot;#f6a735&amp;quot;, &amp;quot;#fbd74d&amp;quot;)
line_colors &amp;lt;- c(&amp;quot;black&amp;quot;)

# Custom theme
my_theme &amp;lt;- list(
 theme(
    plot.background = element_rect(fill =  background_colors[2]),
    text = element_text(color = line_colors),
    panel.background = element_rect(fill =  background_colors[2]),
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.line.y = element_line(color =  line_colors),
    axis.line.x = element_line(color =  line_colors),
    axis.ticks = element_line(color =  line_colors),
    axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0), color = line_colors),
    axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0), color = line_colors),
    legend.position = &amp;quot;bottom&amp;quot;,
    plot.margin = unit(c(0.5,1,0.5,0),&amp;quot;cm&amp;quot;),
    strip.background = element_rect(fill = &amp;quot;white&amp;quot;, color = &amp;quot;black&amp;quot;)
),
  scale_color_manual(
    values = plot_colors,
    guide = guide_legend(direction = &amp;quot;horizontal&amp;quot;, label.hjust = 0.5, title.position = &amp;quot;top&amp;quot;,
                              title.hjust = 0.5)),
  scale_fill_manual(
    values = plot_colors,
    guide = guide_legend(direction = &amp;quot;horizontal&amp;quot;, label.hjust = 0.5, title.position = &amp;quot;top&amp;quot;,
                              title.hjust = 0.5)))&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;import-data-from-api&#34;&gt;Import Data from API&lt;/h1&gt;
&lt;p&gt;Now, after setting everything up, we need to load the data. Since I got some additional data in &lt;code&gt;.csv&lt;/code&gt; format, the importing happens in two steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We import the data from 2017 and 2018 via the Munich Open Data API&lt;/li&gt;
&lt;li&gt;We import the additional data from &lt;code&gt;.csv&lt;/code&gt; format&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;helper-functions&#34;&gt;Helper Functions&lt;/h2&gt;
&lt;p&gt;To fetch the data via API, I wrote some helper functions. This functions will also help us with the preprocessing later. The functions can be found in the &lt;a href=&#34;https://github.com/Somtom/munich-bike-data-analysis&#34;&gt;Github project&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;fetch-data-from-api&#34;&gt;(1) Fetch Data from API&lt;/h2&gt;
&lt;p&gt;The following chunks will import the data for the years 2017 and 2018 (until end of July):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Import muc_opd helper functions
# source(&amp;quot;./muc-opd-helper-functions.R&amp;quot;)

bike_counter_resource_id &amp;lt;- &amp;quot;211e882d-fadd-468a-bf8a-0014ae65a393&amp;quot;
resource_ids &amp;lt;- muc_odp_fetch_bike_count_resource_ids()

resource_ids_15_min &amp;lt;- resource_ids$ids_15_min

resource_ids_daily &amp;lt;- resource_ids$ids_daily&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bike_counter &amp;lt;- muc_odp_fetch_data(resource_id = bike_counter_resource_id, destination = &amp;quot;./data/&amp;quot;)

bike_cnt_15_min &amp;lt;- bind_rows(
  lapply(resource_ids_15_min$resource_id, function(x) muc_odp_fetch_data(resource_id = x,
                                                             destination = &amp;quot;./data/15_min/&amp;quot;))
)

bike_cnt_day &amp;lt;-  bind_rows(
  lapply(resource_ids_daily$resource_id, function(x) muc_odp_fetch_data(resource_id = x,
                                                             destination = &amp;quot;./data/daily/&amp;quot;))
)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;add-additional-data&#34;&gt;(2) Add Additional Data&lt;/h2&gt;
&lt;p&gt;Now we import the additional data from the &lt;code&gt;.csv&lt;/code&gt; files:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;files &amp;lt;- list.files(&amp;quot;./data/additional/&amp;quot;, full.names = TRUE, recursive = TRUE)
files_15_min &amp;lt;- stringr::str_detect(files, &amp;quot;15Min&amp;quot;)

suppressMessages({
  additional_15_min_data &amp;lt;- bind_rows(lapply(files[files_15_min],
                                           read_csv2, 
                                           col_types = cols(.default = &amp;quot;c&amp;quot;)))
additional_daily_data &amp;lt;- bind_rows(lapply(files[!files_15_min],
                                          read_csv2,
                                           col_types = cols(.default = &amp;quot;c&amp;quot;)))
})


# Change date column name to be equal to the API data
names(additional_15_min_data)[1] &amp;lt;- &amp;quot;datum&amp;quot;
names(additional_daily_data)[1] &amp;lt;- &amp;quot;datum&amp;quot;

bike_cnt_day &amp;lt;- bind_rows(additional_daily_data, bike_cnt_day)
bike_cnt_15_min &amp;lt;- bind_rows(additional_15_min_data, bike_cnt_15_min)&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h1&gt;
&lt;p&gt;To prepare the data for the analysis we perform the following steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Convert data types (i.e. &lt;code&gt;character&lt;/code&gt; to &lt;code&gt;POSIXct&lt;/code&gt; for dates)&lt;/li&gt;
&lt;li&gt;Gather the direction columns (&lt;code&gt;richtung_1&lt;/code&gt; and &lt;code&gt;richtung_2&lt;/code&gt;) into one &lt;code&gt;direction&lt;/code&gt; column and convert their names to &lt;code&gt;&#39;into center&#39;&lt;/code&gt; or &lt;code&gt;&#39;out of center&#39;&lt;/code&gt; according to their direction relatively to the city center&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Extract &lt;code&gt;weekday&lt;/code&gt; information and add &lt;code&gt;weekend&lt;/code&gt; indicator column&lt;/li&gt;
&lt;li&gt;Remove duplicates (caused by the additional data)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;convert-types&#34;&gt;(1) Convert Types&lt;/h2&gt;
&lt;p&gt;After importing the data, we need to convert its datatypes to a format which we can work with. We use the predefined helper functions for that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bike_counter &amp;lt;- muc_odp_convert_bike_counter_types(bike_counter)
bike_cnt_15_min &amp;lt;- muc_odp_convert_bike_cnt_15_min_types(bike_cnt_15_min)
bike_cnt_day &amp;lt;- muc_odp_convert_bike_cnt_day_types(bike_cnt_day)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;add-directions-information&#34;&gt;(2) Add Directions Information&lt;/h2&gt;
&lt;p&gt;All tables have two columns which are named &lt;code&gt;richtung_1&lt;/code&gt; and &lt;code&gt;richtung_2&lt;/code&gt;. These stand for the two different directions for which the stations measure the counts. In the &lt;code&gt;bike_counter&lt;/code&gt; table we can find a description which tells us whether the given direction for that station is &lt;code&gt;east&lt;/code&gt;, &lt;code&gt;weast&lt;/code&gt;, &lt;code&gt;north&lt;/code&gt; or &lt;code&gt;south&lt;/code&gt;. We are going to convert the directions to either &lt;code&gt;into center&lt;/code&gt; or &lt;code&gt;out of center&lt;/code&gt; in order to gain more detailed information. In another helper function I precoded this information with a mapping function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bike_cnt_15_min &amp;lt;- muc_odp_add_direction_info(bike_cnt_15_min)
bike_cnt_day &amp;lt;- muc_odp_add_direction_info(bike_cnt_day)
bike_counter &amp;lt;- muc_odp_add_direction_info(bike_counter)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;extract-time-from-uhrzeit_start-english-time-start-and-add-weekend-indicator-column&#34;&gt;(3) Extract Time from &lt;code&gt;uhrzeit_start&lt;/code&gt; (English: Time Start) and Add Weekend Indicator Column&lt;/h2&gt;
&lt;p&gt;In our last preprocessing step we add a &lt;code&gt;time&lt;/code&gt; column to the 15 minutes table. This will hold the information on the &lt;em&gt;time of the day&lt;/em&gt;. Further, we add the &lt;code&gt;weekday&lt;/code&gt; as well as a indicator column which tells us whether the data were measured at the &lt;code&gt;weekend&lt;/code&gt; or during the week.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weekend_days &amp;lt;- c(&amp;quot;Sat&amp;quot;, &amp;quot;Sun&amp;quot;)
bike_cnt_15_min &amp;lt;- bike_cnt_15_min %&amp;gt;% 
  mutate(time = as.numeric(uhrzeit_start - trunc(uhrzeit_start, &amp;quot;days&amp;quot;)),
         weekday = wday(uhrzeit_start, label = TRUE, local = &amp;quot;us&amp;quot;),
         weekend = factor(weekday %in% weekend_days)) 

bike_cnt_day &amp;lt;- bike_cnt_day %&amp;gt;% 
  mutate(weekday = factor(wday(datum, label = TRUE, local = &amp;quot;us&amp;quot;)),
         weekend = factor(weekday %in% weekend_days))

class(bike_cnt_15_min$time) &amp;lt;- &amp;quot;POSIXct&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;remove-duplicates-and-save-converted-data&#34;&gt;(4) Remove Duplicates and Save Converted Data&lt;/h2&gt;
&lt;p&gt;Finally, we remove the duplicates from our data set and save the converted data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bike_cnt_15_min &amp;lt;- unique(bike_cnt_15_min)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;saveRDS(bike_counter, &amp;quot;./2018-11-03-munich-bike-data-info-graphic_files/bike_counter_converted.RDS&amp;quot;)
saveRDS(bike_cnt_15_min, &amp;quot;./2018-11-03-munich-bike-data-info-graphic_files/bike_cnt_15_min_converted.RDS&amp;quot;)
saveRDS(bike_cnt_day, &amp;quot;./2018-11-03-munich-bike-data-info-graphic_files/bike_cnt_day_converted.RDS&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;analysis&#34;&gt;Analysis&lt;/h1&gt;
&lt;p&gt;Now that we are done with preprocessing, we can start with our analysis.&lt;/p&gt;
&lt;h2 id=&#34;time-period-of-our-data-set&#34;&gt;Time Period of Our Data Set&lt;/h2&gt;
&lt;p&gt;First, lets have a look at the time period we are analyzing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(paste0(
  &amp;#39;## 15 Minute Data ##&amp;#39;,
  &amp;#39;\nStart:\t&amp;#39;, min(bike_cnt_15_min$datum),
  &amp;#39;\nEnd:\t&amp;#39;, max(bike_cnt_15_min$datum), 
  &amp;#39;\n\n## Daily Data ##&amp;#39;,
  &amp;#39;\nStart:\t&amp;#39;, min(bike_cnt_day$datum),
  &amp;#39;\nEnd:\t&amp;#39;, max(bike_cnt_day$datum)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ## 15 Minute Data ##
## Start:   2008-06-01
## End: 2018-07-31
## 
## ## Daily Data ##
## Start:   2008-06-01
## End: 2018-07-31&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;missing-data&#34;&gt;Missing Data&lt;/h2&gt;
&lt;p&gt;Very important for every analysis is to be aware of missing data. So let’s have a look at our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bike_cnt_day %&amp;gt;% 
  map(function(x) sum(is.na(x))) %&amp;gt;% 
  bind_cols %&amp;gt;%
  melt(value.name = &amp;quot;na_count&amp;quot;, variable.name = &amp;quot;column&amp;quot;) %&amp;gt;% 
  filter(na_count &amp;gt; 0) %&amp;gt;% knitr::kable(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;column&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;na_count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bewoelkung&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25308&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;sonnenstunden&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25308&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;niederschlag&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25308&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;max_temp&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25308&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;min_temp&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25308&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;id&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;37512&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see that some weather data and ids are missing for the &lt;code&gt;bike_cnt_day&lt;/code&gt; table. These missing values are all from our additional data. Important is that we need to take this into account when we draw conclusions later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bike_cnt_15_min %&amp;gt;% 
  map(function(x) sum(is.na(x))) %&amp;gt;% 
  bind_cols %&amp;gt;%
  melt(value.name = &amp;quot;na_count&amp;quot;, variable.name = &amp;quot;column&amp;quot;) %&amp;gt;% 
  filter(na_count &amp;gt; 0) %&amp;gt;% dim()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No data are missing for &lt;code&gt;bike_cnt_15_min&lt;/code&gt; table.&lt;/p&gt;
&lt;h2 id=&#34;station_map&#34;&gt;Map of Stations&lt;/h2&gt;
&lt;p&gt;To get a first overview of the locations of the 6 counting stations, we create a map of Munich using the &lt;code&gt;ggmap&lt;/code&gt; package and add markers for each station.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;area &amp;lt;- make_bbox(lon = longitude, lat = latitude, data = bike_counter)
area &amp;lt;- area + c(-0.02, -0.01, 0.01, 0.01)

suppressMessages({area_map &amp;lt;- get_map(area,  source = &amp;quot;stamen&amp;quot;)})

ggmap(area_map, extend = &amp;quot;device&amp;quot;, darken = 0.3,
      base_layer = ggplot(aes(x = longitude, y = latitude), data = bike_counter)) +
  geom_point(shape = 21, size = 4, fill = plot_colors[1]) +
  geom_text(aes(label = zaehlstelle), vjust = -1, color = &amp;quot;white&amp;quot;) +
  theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-munich-bike-data-info-graphic_files/figure-html/map-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The stations are spread across Munich. Their locations were planned to be as different as possible but also representative for typical traffic situations (&lt;em&gt;Zorn et. al 2011&lt;/em&gt;).&lt;/p&gt;
&lt;h2 id=&#34;compare-the-data-of-all-stations&#34;&gt;Compare the Data of All Stations&lt;/h2&gt;
&lt;h3 id=&#34;average_counts&#34;&gt;What Are Average 15 Minute and Daily Counts?&lt;/h3&gt;
&lt;p&gt;Now that we have a bird’s eye view on the locations, let’s get an overview on the data by looking at summary stats like average, median and others. First, we start with a summary across all stations. For this purpose we use the &lt;code&gt;summary()&lt;/code&gt; function combined with &lt;code&gt;tidy()&lt;/code&gt; from the &lt;code&gt;broom&lt;/code&gt; package to get everything in one table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(
  c(
    table = &amp;quot;bike_cnt_day&amp;quot;,
    broom::tidy(summary(bike_cnt_day$gesamt)), 
    sd = sd(bike_cnt_day$gesamt)),
  c(
    table = &amp;quot;bike_cnt_15_min&amp;quot;, 
    broom::tidy(summary(bike_cnt_15_min$gesamt)),
    sd = sd(bike_cnt_15_min$gesamt)))  %&amp;gt;% knitr::kable(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;table&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;minimum&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;q1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;q3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;maximum&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bike_cnt_day&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;305&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;862&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1352.91&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1916&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15009&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1471.79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bike_cnt_15_min&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.86&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1157&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23.43&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;On average the stations count &lt;strong&gt;14 bike per 15 minutes&lt;/strong&gt; and &lt;strong&gt;1353 bikes per day&lt;/strong&gt;. When we have a look at the maxima, we see that they are quiet high compared to the means. Especially the 15 minute count maximum is &lt;strong&gt;83&lt;/strong&gt; times as high as the mean. We might find some interesting events being represented by these high counts but we will dive deeper into that later.&lt;/p&gt;
&lt;p&gt;How does the summary data differ if we calculate it per station? We first start with the 15 minute data. To compare the stations we additionally calculate a &lt;code&gt;scaled_median_count&lt;/code&gt; which is the median for a single station divided by the minimum of all station medians.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;station_summary_15_min &amp;lt;- bike_cnt_15_min %&amp;gt;% 
  spread(key = direction, value = count) %&amp;gt;% 
  group_by(zaehlstelle) %&amp;gt;% 
  summarise(avg = round(mean(gesamt, na.rm = T)),
            median_count = median(gesamt, na.rm = T),
            q1 = quantile(gesamt, 0.25, na.rm = T),
            q3 = quantile(gesamt, 0.75, na.rm = T),
            sd = sd(gesamt, na.rm = T),
            percent_sd = round(sd/median_count * 100, 2)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(scaled_median_count = round(median_count / min(median_count))) %&amp;gt;% 
  arrange(desc(median_count)) 

station_summary_15_min  %&amp;gt;% knitr::kable(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;zaehlstelle&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;avg&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median_count&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;q1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;q3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;percent_sd&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;scaled_median_count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Erhardt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;40.17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;211.42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Margareten&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26.98&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179.87&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Arnulf&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14.39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;239.76&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20.61&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;412.13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hirsch&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.68&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1068.43&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Kreuther&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.82&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;482.46&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As we see, Erhardt is the station with the highest median 15 minute count (19 bikes). Its median is 19 times higher than the one of Kreuther. Nevertheless, the standard deviation is very high across all stations, which is an indicator for a lot of intra day deviation.&lt;/p&gt;
&lt;p&gt;How does the per station summary look like for the daily counts?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;station_summary_day &amp;lt;- bike_cnt_day %&amp;gt;% 
  spread(key = direction, value = count) %&amp;gt;% 
  group_by(zaehlstelle) %&amp;gt;% 
  summarise(avg = round(mean(gesamt)),
            median_count = median(gesamt),
            q1 = quantile(gesamt, 0.25),
            q3 = quantile(gesamt, 0.75),
            sd = sd(gesamt),
            percent_sd = round(sd/median_count * 100,2)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(scaled_median_count = round(median_count / min(median_count))) %&amp;gt;% 
  arrange(desc(median_count)) 

station_summary_day  %&amp;gt;% knitr::kable(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;zaehlstelle&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;avg&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median_count&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;q1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;q3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;percent_sd&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;scaled_median_count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Erhardt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3291&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2862.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1646.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4725.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2080.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;72.69&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Margareten&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2385&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2322.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1340.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3277.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1307.51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;56.31&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1332&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1156.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;543.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1891.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1077.79&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;93.23&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Arnulf&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1050&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;896.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;474.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1412.75&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;792.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88.35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hirsch&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;615&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;488.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;141.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;903.75&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;590.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;120.84&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Kreuther&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;283&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;223.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;89.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;400.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;252.36&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;113.17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see a similar picture compared to the 15 minute data. With a median daily bike count of 2862, station Erhardt has the highest daily counts, too. This time the count is 13 higher than the count of Kreuther.&lt;/p&gt;
&lt;h3 id=&#34;wrong_direction&#34;&gt;How Many People Are Riding in the Wrong Direction at Arnulf Street?&lt;/h3&gt;
&lt;p&gt;Station &lt;strong&gt;Arnulf Street&lt;/strong&gt; has a special characteristic: The counter is located on a one way bicycle lane. Thus, normally it should only measure counts for one direction, which is into city center.&lt;/p&gt;
&lt;p&gt;Luckily it is also able to count all bikers who ride into the wrong direction, which we can use for our analysis. So let’s find out which share of the total counts is due to bikers riding the wrong direction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wrong_way_drivers &amp;lt;- bike_cnt_day %&amp;gt;% 
  filter(zaehlstelle == &amp;quot;Arnulf&amp;quot;) %&amp;gt;%
  filter(direction == &amp;quot;out of center&amp;quot;) %&amp;gt;%
  mutate(share = round(count / gesamt * 100, 2)) %&amp;gt;% 
  select(share) %&amp;gt;% unlist() %&amp;gt;% summary() %&amp;gt;% tidy()
wrong_way_drivers  %&amp;gt;% knitr::kable(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;minimum&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;q1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;q3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;maximum&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;na&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.91&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.68&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;101&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The data suggests that on average &lt;strong&gt;5.8 percent&lt;/strong&gt; of daily counted bikes at Arnulf Street were driving on the &lt;strong&gt;wrong side of the road&lt;/strong&gt;. Interestingly, there must have been at least one day where all bikes were riding on the wrong side of the road since we observed a maximum of 100%.&lt;/p&gt;
&lt;h2 id=&#34;count_records&#34;&gt;Count Records&lt;/h2&gt;
&lt;p&gt;Now it is time to come back to the maximum counts we recognized earlier. To get a quick overview before we dive into the story behind some maximum counts, let’s create a top 10 list for both the daily and the 15 minute counts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Daily Counts
top_10_day &amp;lt;- bike_cnt_day %&amp;gt;% 
  spread(key = direction, value = count) %&amp;gt;% 
  arrange(desc(gesamt)) %&amp;gt;% 
  head(10)

top_10_day %&amp;gt;% 
  select(datum, gesamt, zaehlstelle) %&amp;gt;% 
  knitr::kable(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;datum&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;gesamt&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;zaehlstelle&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2012-06-09&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15009&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2010-06-26&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14714&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2010-06-27&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12241&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2013-06-19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9296&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Erhardt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2018-07-26&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9228&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Erhardt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2017-07-05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9207&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Erhardt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2018-07-24&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9179&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Erhardt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2016-06-23&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9113&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Erhardt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2016-07-20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9106&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Erhardt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2018-06-20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9106&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Erhardt&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;That is interesting: Station Olympia takes the top 3 places with counts above 10 thousand, although it only made the third place when we calculated the median counts per station earlier. After some research I found out that these top 3 counts were caused by the same event, namely the &lt;strong&gt;24h Mountainbike Race&lt;/strong&gt;, which takes place in Olympia Park every year. Since the route is changed from year to year, the counting station is not always on the track. Thus, we don’t see these record counts every year.&lt;/p&gt;
&lt;p&gt;Let’s plot the 15 minute data for the events in the two years in which we observed these high counts:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-munich-bike-data-info-graphic_files/figure-html/plot_mtb_race-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2018-11-03-munich-bike-data-info-graphic_files/figure-html/plot_mtb_race-2.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we also create a top 10 list for the 15 minute counts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 15 Minute Counts
top_10_15_min &amp;lt;- bike_cnt_15_min %&amp;gt;% 
  spread(key = direction, value = count) %&amp;gt;% 
  arrange(desc(gesamt)) %&amp;gt;% 
  head(10)

top_10_15_min  %&amp;gt;% 
  select(datum, gesamt, uhrzeit_start, zaehlstelle) %&amp;gt;% knitr::kable(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;datum&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;gesamt&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;uhrzeit_start&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;zaehlstelle&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2017-09-03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1157&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-09-03 13:45:00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2017-09-03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;945&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-09-03 14:00:00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-05-10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;540&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2015-05-10 12:30:00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-05-10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;503&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2015-05-10 12:15:00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-05-10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;455&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2015-05-10 12:45:00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2016-10-16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;421&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2016-10-16 13:45:00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2016-10-16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;409&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2016-10-16 14:00:00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-05-10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;385&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2015-05-10 12:00:00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2012-06-09&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;365&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2012-06-09 23:00:00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-05-10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;363&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2015-05-10 14:45:00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If we have a closer look, we only can find one of our daily count top 10 candidates in this list, namely the &lt;strong&gt;2017-06-09&lt;/strong&gt; on the 6th place. Normally you would suppose the date with highest 15 minute counts to also be listed in the daily count top 10 unless the high counts were due to a short time event. So what happened on the 3rd September 2017 when a 15 minute bike count of &lt;strong&gt;1157&lt;/strong&gt; was recorded?&lt;/p&gt;
&lt;p&gt;This picture from &lt;a href=&#34;https://radlhauptstadt.muenchen.de/&#34;&gt;radlhaupstadt.muenchen.de&lt;/a&gt; gives you a hint:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/rmarkdown-libs/img/ringparade.jpg&#34; alt=&#34;Picture source: radlhauptstadt.muenchen.de&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Picture source: radlhauptstadt.muenchen.de&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;On that day the so called &lt;a href=&#34;https://radlhauptstadt.muenchen.de/mitradeln/muenchner-ringparade/&#34;&gt;Münchner Ringparade&lt;/a&gt; took place. At this event the Munich beltway had been closed temporarily for car traffic while round about 7000 people used this route for a bike tour. The 12 km tour ended in Olympia Park. And this is exactly what we can see in the data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-munich-bike-data-info-graphic_files/figure-html/olympia_record_plot-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;bike_times&#34;&gt;When Are Most People Biking?&lt;/h2&gt;
&lt;h3 id=&#34;pattern-across-a-year&#34;&gt;Pattern Across a Year&lt;/h3&gt;
&lt;p&gt;To answer this question we first try to gain an overview by plotting the development of the daily counts over the years&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-munich-bike-data-info-graphic_files/figure-html/daily_count_over_years-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is definitely a clear seasonal pattern which is similar across all stations. As one would expect, there are higher bike counts during the summer months and lower counts during winter months. However, we also see individual trends or characteristics for some stations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Station &lt;strong&gt;Hirsch&lt;/strong&gt; shows a ongoing trend of rising bike counts from year to year&lt;/li&gt;
&lt;li&gt;At &lt;strong&gt;Arnulf Street’s&lt;/strong&gt; counting station one can see two peak years in 2013 and 2014. These peaks could be due to a railway construction site which affected parts of the trains going into city center.&lt;/li&gt;
&lt;li&gt;Station &lt;strong&gt;Kreuther&lt;/strong&gt; showed some high counts at the beginning of 2013 and then no data for the rest of the year. Here the counting station was affected by a construction site when a separate bicycle path was created. After that, bikes on the sidewalk were not counted anymore. This is why daily bike count level dropped afterwards.&lt;/li&gt;
&lt;li&gt;One can clearly see the events taking place in &lt;strong&gt;Olympia Park&lt;/strong&gt; represented by peaks in the data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When we plot the bike count distribution for each month, we see that &lt;strong&gt;highest median bike counts occur in the month July&lt;/strong&gt;. However, the maximum counts had been recorded in June. These are due to some of the record events mentioned earlier, though. Thus, they need to be interpreted as outliers&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-munich-bike-data-info-graphic_files/figure-html/daily_count_month_wise_distribution-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;day-of-the-week&#34;&gt;Day of the Week&lt;/h3&gt;
&lt;p&gt;When we look at the distribution of total daily counts, we see that counts &lt;strong&gt;during the week&lt;/strong&gt; are &lt;strong&gt;higher than those at the weekends&lt;/strong&gt;. This is most likely due to commuting traffic, which we will try to model later. The maximum counts occurred on Saturdays but are due to single events like the 24h Mountain-bike race we saw earlier.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-munich-bike-data-info-graphic_files/figure-html/bike_count_distribution_week-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;temperature-and-weather-on-days-with-maximum-counts&#34;&gt;Temperature and Weather on Days with Maximum Counts&lt;/h3&gt;
&lt;p&gt;Now we use the weather data we have (at least for the days where they are not missing) and try to characterize the days with highest bike counts. We look at the distribution of measured values within each weather variable &lt;strong&gt;for the month July&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To do so we group the bike counts into 4 categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lowest 25%:&lt;/strong&gt; Daily bike counts which lie between the &lt;strong&gt;minimum&lt;/strong&gt; count and the &lt;strong&gt;25% percentile&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lower Middle:&lt;/strong&gt; Daily bike counts which lie between the &lt;strong&gt;25% percentile&lt;/strong&gt; and the &lt;strong&gt;median&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upper Middle:&lt;/strong&gt; Daily bike counts which lie between the &lt;strong&gt;median&lt;/strong&gt; and the &lt;strong&gt;75% percentile&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Highest 25%:&lt;/strong&gt; Daily bike counts which lie between the &lt;strong&gt;75% percentile&lt;/strong&gt; and the &lt;strong&gt;maximum&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We then will have a closer look into the &lt;strong&gt;“Highest 25%”&lt;/strong&gt; group and the weather parameters measured on these days.&lt;/p&gt;
&lt;p&gt;To make the data comparable between stations, we perform the following steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Removing outliers by filtering for bike counts below the 99% percentile for each station&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Scaling the daily counts from each station by dividing the counts by each station’s maximum count&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Using the scaled counts (values between 0 and 1) to assign bike count categories&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;breaks &amp;lt;- bike_cnt_day %&amp;gt;% 
  filter(month(datum) == 7) %&amp;gt;% 
  spread(key = direction, value = count) %&amp;gt;%
  group_by(zaehlstelle) %&amp;gt;% 
  # Calculate 99% percentile per station in order to filter outlieres 
  mutate(percentile_99 = quantile(gesamt, 0.99))  %&amp;gt;%
  filter(gesamt &amp;lt; percentile_99) %&amp;gt;% 
  # Scale to make compareable across stations
  mutate(gesamt_scaled = gesamt / max(gesamt)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  # Create count classes
  summarise(count_q1 = quantile(gesamt_scaled, 0.25),
         count_q2 = quantile(gesamt_scaled, 0.5),
         count_q3 = quantile(gesamt_scaled, 0.75),
         max_count = max(gesamt_scaled)) %&amp;gt;% unlist()

labels &amp;lt;- c(&amp;quot;Lowest 25%\n[0-q1]&amp;quot;, 
            &amp;quot;Lower Middle\n(q1-q2]&amp;quot;,
            &amp;quot;Upper Middle\n(q2-q3]&amp;quot;, 
            &amp;quot;Highest 25%\n(q3-max]&amp;quot;)

bike_cnt_day_july_scaled &amp;lt;- bike_cnt_day %&amp;gt;% 
  filter(month(datum) == 7) %&amp;gt;% 
  spread(key = direction, value = count) %&amp;gt;%
  group_by(zaehlstelle) %&amp;gt;% 
  # Calculate 99% percentile per station in order to filter outlieres 
  mutate(percentile_99 = quantile(gesamt, 0.99))  %&amp;gt;%
  filter(gesamt &amp;lt; percentile_99) %&amp;gt;% 
  # Scale to make compareable across stations
  mutate(gesamt_scaled = gesamt / max(gesamt)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(cat = cut(gesamt_scaled, breaks = c(0, breaks), include.lowest = T, labels = labels)) %&amp;gt;%
  select(cat, bewoelkung, sonnenstunden, niederschlag, max_temp, min_temp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-munich-bike-data-info-graphic_files/figure-html/temperature_and_weather_distribution_for_maximum_days-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When we look at each weather parameter, we can see differences in value distributions between the 4 bike count categories. However these differences are not always that clear. But, there are at least trends for all parameters except one, namely the minimum temperature, where mostly the variance differs. The other parameters don’t show great surprises: You would, for example, expect the precipitation to be very low on days with a lot of bikes being counted.&lt;/p&gt;
&lt;p&gt;In order to get some numbers to describe the days with highest counts, let’s look into a summary for each weather category at days with bike counts within the “Highest 25%” group in July.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_mode &amp;lt;- function(x) {
  ux &amp;lt;- na.omit(unique(x))
  tab &amp;lt;- tabulate(match(x, ux))
  ux[tab == max(tab)][1]
}

# Characterise the metrics for the maximum category within july
bike_cnt_day_july_scaled %&amp;gt;% 
  group_by(cat) %&amp;gt;% 
  summarise(max_temp = list(cbind(broom::glance(summary(max_temp)), mode = get_mode(max_temp))),
            min_temp = list(cbind(broom::glance(summary(min_temp)), mode = get_mode(min_temp))),
            bewoelkung = list(cbind(broom::glance(summary(bewoelkung)), mode = get_mode(bewoelkung))),
            sonnenstunden = list(cbind(broom::glance(summary(sonnenstunden)), mode = get_mode(sonnenstunden))),
            niederschlag = list(cbind(broom::glance(summary(niederschlag)), mode = get_mode(niederschlag)))) %&amp;gt;%
  gather(key = metric, value = value, -cat)  %&amp;gt;% 
  unnest() %&amp;gt;%   
  filter(cat == &amp;quot;Highest 25%\n(q3-max]&amp;quot;) %&amp;gt;% mutate(metric = plyr::revalue(metric, 
                                c(&amp;quot;bewoelkung&amp;quot; = &amp;quot;Cloudiness [%]&amp;quot;,
                                  &amp;quot;max_temp&amp;quot; = &amp;quot;Maximum Temperature [°C]&amp;quot;,
                                  &amp;quot;min_temp&amp;quot; = &amp;quot;Minimum Temperature [°C]&amp;quot;,
                                  &amp;quot;niederschlag&amp;quot; = &amp;quot;Precipitation  [ml]&amp;quot;,
                                  &amp;quot;sonnenstunden&amp;quot; = &amp;quot;Hours of Sunshine [h]&amp;quot;))) %&amp;gt;% 
  select(-na, -cat)  %&amp;gt;% knitr::kable(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;metric&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;minimum&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;q1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;q3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;maximum&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mode&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Maximum Temperature [°C]&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28.24&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;36.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Minimum Temperature [°C]&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15.62&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cloudiness [%]&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;40.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43.73&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;71.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;95.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;75.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hours of Sunshine [h]&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.89&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Precipitation [ml]&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Great! We can take these median values for the info-graphic to give a statement for a typical biking day:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Days with highest counts lie in &lt;strong&gt;July&lt;/strong&gt;. The temperature on these days is &lt;strong&gt;15.7 - 28.2°C&lt;/strong&gt; and the &lt;strong&gt;sun&lt;/strong&gt; is shining for &lt;strong&gt;12 hours&lt;/strong&gt; with &lt;strong&gt;scattered clouds&lt;/strong&gt; but &lt;strong&gt;without&lt;/strong&gt; any &lt;strong&gt;precipitation.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;commuting&#34;&gt;Modeling Commuting Traffic&lt;/h2&gt;
&lt;h3 id=&#34;differences-between-weekend-counts-and-counts-during-week&#34;&gt;Differences Between Weekend Counts and Counts During Week&lt;/h3&gt;
&lt;p&gt;We already got some nice information about the bike traffic in Munich. Our next aim is to use the 15 minute data to model the commuting traffic. We start with a bird’s eye overview by plotting the median 15 minute counts across all stations for weekend and working days.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-munich-bike-data-info-graphic_files/figure-html/15_min_plot_weekend_week-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the 15 minute count patterns clearly differ between weekend days and week days. There are two clear commuting peaks during the week. However, there still seems to be regular traffic between these peaks.&lt;br /&gt;
We used the median 15 minute bike count across all stations for the previous plot. But, will we still see the same pattern if we look at each station separately?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-munich-bike-data-info-graphic_files/figure-html/15_min_plot_weekend_week_per_station-1.png&#34; width=&#34;1248&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Indeed, the pattern is similar across all stations. Nevertheless, stations show different commuting direction characteristics. The stations can be assigned to either one of these 3 groups:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Station with traffic mainly in the direction of the city center in the morning and back way in the afternoon:
&lt;ul&gt;
&lt;li&gt;Erhardt&lt;/li&gt;
&lt;li&gt;Hirsch&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Stations with traffic mainly out of the city center in the morning and back in the afternoon
&lt;ul&gt;
&lt;li&gt;Kreuther&lt;/li&gt;
&lt;li&gt;Margareten&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Stations with similar patterns for traffic into- and out of city center
&lt;ul&gt;
&lt;li&gt;Olympia&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We cannot really tell for Arnulf Street but it looks like the peaks in the morning and in the afternoon are similar for the only measured direction (into center). This would make it fall into class 3.&lt;/p&gt;
&lt;h3 id=&#34;the-model&#34;&gt;The Model&lt;/h3&gt;
&lt;p&gt;To create a really basic model of the commuting traffic, we make the following assumption: &lt;em&gt;Traffic during week is the result of commuting traffic and the normal traffic which is represented by the weekend data.&lt;/em&gt; Under this assumption subtracting the 15 minute weekend counts from the 15 minute counts during the week gives us the commuting part. We are going to create a &lt;code&gt;loess&lt;/code&gt; model for each the weekend and the week counts for that purpose.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bike_cnt_15_min_separated &amp;lt;- bike_cnt_15_min %&amp;gt;% 
  mutate(Weekend = factor(ifelse(weekend == TRUE, &amp;quot;Weekend&amp;quot;, &amp;quot;Week&amp;quot;))) %&amp;gt;% 
  group_by(Weekend, time) %&amp;gt;% 
  summarise(median = median(gesamt)) %&amp;gt;% 
  spread(key = Weekend, value = median) %&amp;gt;% 
  mutate(Commuting = Week - Weekend) %&amp;gt;% 
  gather(key = week_cat, value = median_count, Commuting, Weekend, Week) %&amp;gt;% 
  mutate(week_cat = factor(week_cat, levels = c(&amp;quot;Weekend&amp;quot;, &amp;quot;Week&amp;quot;, &amp;quot;Commuting&amp;quot;)),
         commuting_cat = factor(ifelse(week_cat == &amp;quot;Commuting&amp;quot;,
                                       &amp;quot;Commuting Traffic&amp;quot;,
                                       &amp;quot;Normal Traffic&amp;quot;),
                                levels = c(&amp;quot;Normal Traffic&amp;quot;, &amp;quot;Commuting Traffic&amp;quot;))) 

tmp &amp;lt;- bike_cnt_15_min_separated %&amp;gt;% 
  mutate(median_count = ifelse(median_count &amp;lt; 0, 0, median_count),
         x = as.numeric(time)) 

fit_commuting &amp;lt;- loess(median_count~x, data = tmp, subset = c(tmp$week_cat == &amp;quot;Commuting&amp;quot;), span = 0.2)
fit_weekend &amp;lt;- loess(median_count~x, data = tmp, subset = c(tmp$week_cat == &amp;quot;Weekend&amp;quot;), span = 0.2)
fit_week &amp;lt;- loess(median_count~x, data = tmp, subset = c(tmp$week_cat == &amp;quot;Week&amp;quot;), span = 0.2)

bike_cnt_15_min_modeled &amp;lt;- data.frame(x = seq(min(tmp$x), max(tmp$x)), 60) %&amp;gt;% 
  add_predictions(fit_commuting, &amp;quot;pred_commuting&amp;quot;) %&amp;gt;%
  add_predictions(fit_weekend, var = &amp;quot;pred_weekend&amp;quot;) %&amp;gt;% 
  add_predictions(fit_week, var = &amp;quot;pred_week&amp;quot;) %&amp;gt;%
  # Adjust for 0 values
  mutate(time = as.POSIXct(x, origin = min(tmp$time)),
         afternoon = x &amp;gt; 43200,
         pred_commuting = ifelse(pred_commuting &amp;lt; 0, 0, pred_commuting),
         pred_weekend = ifelse(pred_weekend &amp;lt; 0, 0, pred_weekend),
         pred_week = ifelse(pred_week &amp;lt; 0, 0, pred_week)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-munich-bike-data-info-graphic_files/figure-html/commuting_model_plot-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When we use this modeling approach, the predicted commuting traffic would look like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-munich-bike-data-info-graphic_files/figure-html/commuting_week_and_weekend_plot-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The result looks quiet good. We are able to see the two peaks, one in the morning and another one in the afternoon.&lt;/p&gt;
&lt;h3 id=&#34;peak-commuting-times-according-to-model&#34;&gt;Peak Commuting Times According to Model&lt;/h3&gt;
&lt;p&gt;Now, that we have a model for commuting traffic, when are the peak times?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;morning &amp;lt;- bike_cnt_15_min_modeled %&amp;gt;% filter(!afternoon)
afternoon &amp;lt;- bike_cnt_15_min_modeled %&amp;gt;% filter(afternoon)
# Morning Peak
morning_peak &amp;lt;- lubridate::with_tz(morning[which.max(morning$pred_commuting), ]$time, &amp;quot;UTC&amp;quot;)
cat(paste0(&amp;quot;Morning peak:\n&amp;quot;, format(morning_peak, &amp;quot;%H:%M:%S&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Morning peak:
## 08:03:27&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Afternoon Peak
afternoon_peak &amp;lt;- lubridate::with_tz(afternoon[which.max(afternoon$pred_commuting), ]$time, &amp;quot;UTC&amp;quot;)
cat(paste0(&amp;quot;Afternoon peak:\n&amp;quot;, format(afternoon_peak, &amp;quot;%H:%M:%S&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Afternoon peak:
## 17:54:48&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;time_diff &amp;lt;- difftime(afternoon_peak,
                      morning_peak, units = &amp;quot;hours&amp;quot;, tz = &amp;quot;UTC&amp;quot;)

hours &amp;lt;- floor((time_diff))
mins &amp;lt;- floor(as.numeric(time_diff - hours, units = &amp;quot;mins&amp;quot;))
secs &amp;lt;- as.numeric(time_diff - hours, units = &amp;quot;secs&amp;quot;) - mins * 60

peak_to_peak_duration &amp;lt;- paste0(hours, &amp;quot;h &amp;quot;, mins, &amp;quot;min &amp;quot;, round(secs), &amp;quot;sec&amp;quot;)
cat(paste0(&amp;quot;Peak to peak duration:\n&amp;quot;, peak_to_peak_duration))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Peak to peak duration:
## 9h 51min 21sec&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The morning peak is around 08:03:27 and the afternoon commuting peak is around 17:54:48. The peak to peak duration is 9h 51min 21sec.&lt;/p&gt;
&lt;p&gt;Let’s go on and compare the two peak times a little bit further. When I was looking for a way to characterize the duration of a peak, I found a metric called &lt;a href=&#34;https://en.wikipedia.org/wiki/Full_width_at_half_maximum&#34;&gt;Full Duration Half Maximum (FDHM)&lt;/a&gt;. It measures the duration between the points of y axis that are half the maximum amplitude.&lt;/p&gt;
&lt;p&gt;We use this method for our data to get a FDHM for both peaks&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calculate_FDHM_stats &amp;lt;- function(y, time, period_start, period_end) {
  period_filter &amp;lt;-  time &amp;gt;= period_start &amp;amp; time &amp;lt;= period_end
  y &amp;lt;- subset(y, period_filter)
  time &amp;lt;- subset(time, period_filter)
  max_y &amp;lt;- max(y)
  
  # Calculate index of intersection points of max_y/2 and y within period
  i &amp;lt;- c(FALSE, diff(max_y/2 &amp;gt; y) != 0)
  times &amp;lt;- time[i]
  y_vals &amp;lt;- y[i]
  
  list(
    times = times,
    y_vals = y_vals,
    max_y = max_y,
    max_y_x_val = time[which.max(y)],
    fdhm = round(difftime(times[2], times[1], units = &amp;quot;mins&amp;quot;))
  )
}

day_start &amp;lt;- as.POSIXct(&amp;quot;1970-01-01 00:00:00&amp;quot;)
noon &amp;lt;- as.POSIXct(&amp;quot;1970-01-01 12:00:00&amp;quot;)
day_end &amp;lt;- as.POSIXct(&amp;quot;1970-01-01 23:59:59&amp;quot;)

fdhm_stats_morning &amp;lt;- calculate_FDHM_stats(bike_cnt_15_min_modeled$pred_commuting,
                     bike_cnt_15_min_modeled$time,
                     day_start,
                     noon)
fdhm_stats_afternoon &amp;lt;- calculate_FDHM_stats(bike_cnt_15_min_modeled$pred_commuting,
                     bike_cnt_15_min_modeled$time,
                     noon,
                     day_end)

cat(paste0(&amp;quot;Morning FDHM: \n&amp;quot;, fdhm_stats_morning$fdhm, &amp;quot; Minutes\n&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Morning FDHM: 
## 140 Minutes&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(paste0(&amp;quot;Afternoon FDHM: \n&amp;quot;, fdhm_stats_afternoon$fdhm, &amp;quot; Minutes\n&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Afternoon FDHM: 
## 197 Minutes&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(paste0(&amp;quot;Afternoon FDHM is &amp;quot;, fdhm_stats_afternoon$fdhm - fdhm_stats_morning$fdhm,
           &amp;quot; Minutes longer&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Afternoon FDHM is 57 Minutes longer&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We create a plot to give a better visual understanding of these metrics:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bike_cnt_15_min_modeled %&amp;gt;% 
  ggplot() +
  geom_line(aes(x = time, y = pred_commuting), color = plot_colors[2]) +
  annotate(geom = &amp;quot;ribbon&amp;quot;, x = fdhm_stats_morning$times, ymax = Inf, ymin = -Inf, alpha = 0.1,
           fill = plot_colors[2]) +
  annotate(geom = &amp;quot;ribbon&amp;quot;, x = fdhm_stats_afternoon$times, ymax = Inf, ymin = -Inf, alpha = 0.1,
           fill = plot_colors[2]) +
  annotate(geom = &amp;quot;text&amp;quot;, 
           x = fdhm_stats_morning$times[1] - 2*60*60, 
           y = max(bike_cnt_15_min_modeled$pred_commuting),
           label = paste0(&amp;quot;Morning FDHM:\n&amp;quot;, fdhm_stats_morning$fdhm, &amp;quot; Minutes&amp;quot;),
           hjust = 0.5,
           vjust = 1,
           size = 3) +
  annotate(geom = &amp;quot;text&amp;quot;, 
           x = fdhm_stats_afternoon$times[2] + 2*60*60, 
           y = max(bike_cnt_15_min_modeled$pred_commuting),
           label = paste0(&amp;quot;Afternoon FDHM:\n&amp;quot;, fdhm_stats_afternoon$fdhm, &amp;quot; Minutes&amp;quot;),
           hjust = 0.5,
           vjust = 1,
           size = 3) +
  scale_x_datetime(labels = scales::date_format(&amp;quot;%H:%M&amp;quot;), date_breaks = &amp;quot;2 hours&amp;quot;) +
  xlab(&amp;quot;Time of the Day&amp;quot;) +
  ylab(&amp;quot;Bikes per 15 Minutes&amp;quot;) +
  ggtitle(&amp;quot;According to the FDHM* metric the afternoon commuting peak lasts longer than the morning peak&amp;quot;,
          &amp;quot;*Full Duration Half Maximum: Duration between points of y axis that are half the maximum amplitude&amp;quot;) +
  my_theme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-03-munich-bike-data-info-graphic_files/figure-html/fdhm_plot-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As a last part of the peak analysis we compare the total bike counts within the modeled peaks to see whether those are comparable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; bike_cnt_15_min %&amp;gt;% 
  mutate(weekend = factor(ifelse(weekend == TRUE, &amp;quot;Weekend&amp;quot;, &amp;quot;Week&amp;quot;))) %&amp;gt;% 
  group_by(weekend, time) %&amp;gt;% 
  summarise(median = median(gesamt),
            q1 = quantile(gesamt, 0.25),
            q3 = quantile(gesamt, 0.75))  %&amp;gt;% 
  mutate(peak = ifelse(time &amp;lt; noon, 
                       &amp;quot;morning&amp;quot;,
                       &amp;quot;afternoon&amp;quot;)) %&amp;gt;%  
  filter(weekend == &amp;quot;Week&amp;quot;) %&amp;gt;% 
  group_by(peak) %&amp;gt;% 
  summarise(bike_count = sum(median))  %&amp;gt;% knitr::kable(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;peak&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;bike_count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;afternoon&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;612&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;morning&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;302&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see that the counts in the afternoon are twice as high as the morning counts. If people only would drive to work in the morning and back home in the afternoon, one would expect these two values to be equal. However, one explanation for this inequality could be that, once people finished their work, they don’t drive home immediately but go for other activities. These activities could be sports, meeting friends or doing the grocery, for instance. Thus, they would perhaps pass the counting stations more than once or pass a counting station which normally doesn’t lie on their commuting route. This could lead to the higher counts in the afternoon and would also be a explanation for some counts occurring very late in the evening (past 10pm).&lt;/p&gt;
&lt;h3 id=&#34;share-of-modeled-commuting-traffic&#34;&gt;Share of Modeled Commuting Traffic&lt;/h3&gt;
&lt;p&gt;Finally, we use our model to compute the share of commuting traffic during the week. To do so, we calculate the difference of daily counts between weekdays and weekend as well as the share of this difference on weekday counts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;commuting_share &amp;lt;- bike_cnt_day %&amp;gt;% 
  mutate(weekend = ifelse(weekend == TRUE, &amp;quot;weekend&amp;quot;, &amp;quot;week&amp;quot;)) %&amp;gt;% 
  spread(key = direction, value = count) %&amp;gt;% 
  group_by(zaehlstelle, weekend) %&amp;gt;% 
  summarise(median_count = median(gesamt)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  spread(key = weekend, value = median_count) %&amp;gt;% 
  mutate(diff_week_weekend = week - weekend,
         share_commuting = diff_week_weekend / week) %&amp;gt;% 
  arrange(zaehlstelle, desc(share_commuting)) %&amp;gt;% 
  mutate(mean_share_commuting = mean(share_commuting),
         median_share_commuting = median(share_commuting))

commuting_share %&amp;gt;% 
  select(zaehlstelle, share_commuting, mean_share_commuting, median_share_commuting) %&amp;gt;% 
  knitr::kable(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;zaehlstelle&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;share_commuting&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean_share_commuting&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median_share_commuting&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Arnulf&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.47&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Erhardt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hirsch&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Kreuther&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.44&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Margareten&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.43&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Olympia&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.44&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The modeled share of commuting traffic varies between the station and ranges from 29% to 51 %. The mean commuting share across all stations lies around 42%.&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;Phew, that was a really long one! Thanks you for your interest in the analysis behind the info-graphic. I hope you got some interesting insights into the Munich bike traffic and found it as exciting as I did.&lt;/p&gt;
&lt;p&gt;Let’s quickly sum up our analysis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To get a first overview, we created a map with all the stations.&lt;/li&gt;
&lt;li&gt;We created a summary across all stations and for each station separately where we looked at the average 15 minute and daily counts, for instance. Further, we found out that Erhardt is the most frequented station and Kreuther has the fewest bike counts.&lt;/li&gt;
&lt;li&gt;We found out that 5.8% of daily counted bikes at Arnulf Street arise from people driving at the wrong side of the road&lt;/li&gt;
&lt;li&gt;We had a closer look on bike count records and found out about events like Münchner Ringparade and the 24h Mountain Bike Race in Munich.&lt;/li&gt;
&lt;li&gt;Additionally we tried to figure out when most people are biking and how these days look like.&lt;/li&gt;
&lt;li&gt;Finally we modeled the commuting traffic. Even if our commuting model is by far not perfect, it gave us some interesting insights on peak commuting times.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before I finish, I want to thank the Munich Open Data Team - especially Frank Börger and his coworker Lukas Raffl - for their great support and for providing me with inspirations and additional data. Also, great thanks to my friends and colleagues Ryan, Stephie, Flo and my girlfriend Samira for proof reading!&lt;/p&gt;
&lt;p&gt;It’s getting winter now as I am writing this post, but I hope that your new knowledge about bike counts in Munich motivates you to also go out and ride your bike. Maybe you are living in Munich and want to add a few counts to the stations. If not, perhaps your city also has similar counting stations which you can check out. Try to do some research!&lt;/p&gt;
&lt;p&gt;As always: If you want to add something or give me feedback just send me a message. I highly appreciate it! Also feel free to share the info-graphic with your friends, but please make sure to link the post in the resources!&lt;/p&gt;
&lt;h1 id=&#34;literature&#34;&gt;Literature&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Zorn, Elisabeth; Hager, Gerhard; Wöppel, Hans-Dieter: Datenerhebungen zum Radverkehr in München Jg.55, Nr.1, 2011 (p.32 - 39)&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;hr /&gt;
&lt;center&gt;
&lt;button class=&#34;btn btn-primary btn-outline&#34; id=&#34;btn-toggle-analysis&#34; onclick=&#34;showAnalysis()&#34;&gt;
Show Analysis
&lt;/button&gt;
&lt;/center&gt;
&lt;hr /&gt;
&lt;script&gt; 
function showAnalysis() {
    var btn = document.getElementById(&#39;btn-toggle-analysis&#39;);
    var x = document.getElementById(&#39;analysis&#39;);
        if (x.style.display === &#39;none&#39;) {
            btn.firstChild.data = &#34;Hide Analysis&#34;;
            x.style.display = &#39;block&#39;;
        } else {
            btn.firstChild.data = &#34;Show Analysis&#34;
            x.style.display = &#39;none&#39;;
    }
}
&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>USA School Shootings</title>
      <link>/post/usa-school-shootings/</link>
      <pubDate>Mon, 28 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/usa-school-shootings/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/leaflet/leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/leaflet/leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/leafletfix/leafletfix.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/Proj4Leaflet/proj4-compressed.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/Proj4Leaflet/proj4leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/rstudio_leaflet/rstudio_leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/leaflet-binding/leaflet.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;section&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;22…&lt;/h1&gt;
&lt;p&gt;… this is the count of school shootings which took place in USA within the first 21 weeks in 2018. Statistically this means that &lt;strong&gt;there has been more than 1 shooting per week.&lt;/strong&gt; The &lt;a href=&#34;https://www.washingtonpost.com/news/politics/wp/2018/05/18/2018-has-been-deadlier-for-schoolchildren-than-service-members/?noredirect=on&amp;amp;utm_term=.9ef6aff4ac6d&#34;&gt;Washington Post&lt;/a&gt; wrote that this led to more deaths at schools than members of the US military have been killed while being deployed this year.&lt;/p&gt;
&lt;p&gt;We hear about those shootings quite often in the news, and after almost every of theses a new discussion about the USA gun laws arises. Nevertheless, I do not want to get into theses discussions here but rather share some of my research with you. I was wondering if I could visualize USA school shootings on a map and decided to create a interactive dashboard to explore the data a little bit further. I found the &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_school_shootings_in_the_United_States&#34;&gt;School Shootings in the United States&lt;/a&gt; Wikipedia article which I used as a datasource for my project.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://somtom.shinyapps.io/usa_school_shootings_shiny/&#34;&gt;&lt;img src=&#34;../additional_data/images/school-shootings-dashboard.png&#34; alt=&#34;Dashboard Screenshot&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We need to keep in mind that, hidden behind all these data and statistics, there are people. People who died, people who got injuried, people who have been crying for their friends and family members and even more. Feel free to explore the data by your own to get a feeling for all those incidents and victims:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://somtom.shinyapps.io/usa_school_shootings_shiny/&#34;&gt;Link to R-Shiny USA School Shootings Dashboard&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;One word of caution at this point:&lt;/strong&gt; We cannot be sure that the article has documented every single shooting, but the Wikipedia community is working hard to keep the records up-to-date and adds new entries really quickly. Nevertheless, the present dataset does not raise the claim to be complete and to include every incidents ever happened.&lt;/p&gt;
&lt;p&gt;The code for the dashboard can be found on my GitHub account:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Somtom/shiny_dasbhoard_school_shootings_USA&#34;&gt;R-Shiny USA School Shootings Dashboard - Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For those who are interested in the data cleaning and preparation, I will go trough this process step by step in the rest of the blog post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;environment-setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Environment Setup&lt;/h1&gt;
&lt;p&gt;In order to read the html tables from the Wikipedia article we are going to use the &lt;code&gt;rvest&lt;/code&gt; package. To clean the data we will use the &lt;code&gt;tidyverse&lt;/code&gt; package, and to built a interactive map later we are going to use the &lt;code&gt;leaflet&lt;/code&gt; package. Further we will need &lt;code&gt;ggmap&lt;/code&gt; for geocoding.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if (!require(rvest)) {
  install.packages(&amp;quot;rvest&amp;quot;)
}
if (!require(ggmap)) {
  devtools::install_github(&amp;quot;dkahle/ggmap&amp;quot;)
}

if (!require(leaflet)) {
  install.packages(&amp;quot;leaflet&amp;quot;)
}
library(rvest)
library(tidyverse)
library(ggmap)
library(leaflet)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;load-data-from-wikipedia&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Load Data from Wikipedia&lt;/h1&gt;
&lt;p&gt;To extract the data directly from the Wikipedia article we use the &lt;code&gt;rvest&lt;/code&gt; package from Hadley Wickham. This package makes it easy to scrape data from html web pages. You can find some further information on its &lt;a href=&#34;https://github.com/hadley/rvest&#34;&gt;Github page&lt;/a&gt; or on &lt;a href=&#34;http://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/&#34;&gt;this small blogpost&lt;/a&gt; from Hadley Wickham.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;url &amp;lt;- &amp;quot;https://en.wikipedia.org/wiki/List_of_school_shootings_in_the_United_States&amp;quot;

# Read html and save it to the dashboard data folder for download
articleHTML &amp;lt;- url %&amp;gt;% 
  read_html()

write_html(articleHTML,
           &amp;quot;./usa_school_shootings_shiny/data/List_of_school_shootings_in_the_United_States.html&amp;quot;)

# Extract tables and save them to a list
table_list &amp;lt;- articleHTML %&amp;gt;% 
  html_nodes(&amp;quot;table&amp;quot;) %&amp;gt;% 
  html_table()

# Convert all columns to character to avoid errors because of unclean data when binding rows
dt &amp;lt;- map(table_list, function(x) map(x,as.character)) %&amp;gt;% 
  bind_rows()

# Save raw data to dashbaord data folder
saveRDS(dt, &amp;quot;./usa_school_shootings_shiny/data/raw.RDS&amp;quot;)

head(dt)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-preparation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Preparation&lt;/h1&gt;
&lt;div id=&#34;data-cleaning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Cleaning&lt;/h2&gt;
&lt;p&gt;We need to perform some data processing and cleaning steps so that we can use it for our dashboard. This steps include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remove duplicated part of &lt;code&gt;Date&lt;/code&gt; (i.e. &lt;code&gt;000000001764-07-26-0000July 26, 1764&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Remove duplicated &lt;code&gt;Location&lt;/code&gt; after “!”-character (i.e. &lt;code&gt;Greencastle, Pennsylvania !Greencastle, Pennsylvania&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Convert &lt;code&gt;Injuries&lt;/code&gt; and &lt;code&gt;Deaths&lt;/code&gt; to integer (characters like “?”, “1+”, will be converted to NA)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dt &amp;lt;- dt %&amp;gt;% 
  mutate(
    # If Date contains &amp;quot;-0000&amp;quot; then remove the first part from it (first 24 characters)
    Date = ifelse(str_detect(Date, &amp;quot;-0000&amp;quot;), 
                  str_sub(Date, 24),
                  Date),
    # Convert Date to Date type
    Date = parse_date(Date, format = &amp;quot;%B %d, %Y&amp;quot;, locale = locale(&amp;quot;en&amp;quot;)),
    year = as.integer(format(Date, &amp;quot;%Y&amp;quot;)),
    century = as.integer(format(Date, &amp;quot;%C&amp;quot;)),
    decade = floor(year/10)*10,
    # If Location contains &amp;quot;!&amp;quot;, then remove part after that character
    Location = ifelse(str_detect(Location, &amp;quot;!&amp;quot;),
                      str_sub(Location, 1, str_locate(Location, &amp;quot; !&amp;quot;)[,1] - 1),
                      Location),
    # Count words in Location for correct State extraction
    words_in_location = str_count(Location, &amp;#39;\\w+&amp;#39;),
    ### Extract State from Location variable ###
    # If City provided (words_in_location &amp;gt; 1), split City and State to only get State
    State = ifelse(words_in_location &amp;gt; 1,
                   str_split_fixed(Location, &amp;quot;,&amp;quot;, n = 2)[,2],
                   Location),
    # Trim whitespace and remove &amp;quot;.&amp;quot; from abbreviations
    State = gsub(&amp;quot;\\.&amp;quot;, &amp;quot;&amp;quot;, trimws(State)),
    # Correct state abbreviations using the R state.abb and state.name dataset
    State = ifelse(State %in% state.abb,
                   state.name[match(State, state.abb)],
                   State),
    # Convert Deaths and Injuries to integer
    ### End: Extract State from Location variable ###
    Deaths = as.integer(Deaths),
    Injuries = as.integer(Injuries),
    # Create html popup message for later plot
    popup = paste0(&amp;quot;&amp;lt;b&amp;gt;Date: &amp;quot;, Date, &amp;quot;&amp;lt;/b&amp;gt;&amp;lt;br/&amp;gt;&amp;quot;,
                   &amp;quot;&amp;lt;b&amp;gt;Deaths: &amp;quot;, Deaths, &amp;quot;&amp;lt;/b&amp;gt;&amp;lt;br/&amp;gt;&amp;quot;,
                   &amp;quot;&amp;lt;b&amp;gt;Injuries: &amp;quot;, Injuries,&amp;quot;&amp;lt;/b&amp;gt;&amp;lt;br/&amp;gt;&amp;quot;,
                   &amp;quot;&amp;lt;br/&amp;gt;&amp;quot;,
                   &amp;quot;&amp;lt;b&amp;gt;Description: &amp;lt;/b&amp;gt;&amp;lt;br/&amp;gt;&amp;quot;,
                   Description)
  ) %&amp;gt;% 
  select(-words_in_location)

dt&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;geocoding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Geocoding&lt;/h2&gt;
&lt;p&gt;Now that we have cleaned the data, we can convert the &lt;code&gt;Location&lt;/code&gt; column to latitude and longitude data for our plot by using the &lt;code&gt;geocode()&lt;/code&gt; function from the &lt;code&gt;ggmap&lt;/code&gt; package. You’ll find a small introduction in my previous &lt;a href=&#34;https://somtom.github.io/&#34;&gt;blogpost&lt;/a&gt;, where I used the package to geocode the addresses from San Francisco Police Departments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;api_key &amp;lt;- read_lines(&amp;quot;./additional_data/api-key.txt&amp;quot;) #you would need to insert your api key here
register_google(api_key, account_type = &amp;quot;standard&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get location from Google Maps Geocoding API
locations &amp;lt;- geocode(dt$Location, messaging = FALSE)

# add latitude and longitude data to our data frame
dt &amp;lt;- bind_cols(dt, locations)

saveRDS(dt, &amp;quot;./usa_school_shootings_shiny/data/cleaned.RDS&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;some-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Some plots&lt;/h1&gt;
&lt;div id=&#34;leaflet-map&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Leaflet Map&lt;/h2&gt;
&lt;p&gt;Having finished the data preprocessing we are going to use the &lt;code&gt;leaflet&lt;/code&gt; package to create a interactive map. I also used this package within the Shiny dashboard. Here we will show all incidents in year 2018&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;leafletColors &amp;lt;- colorFactor(palette = c(Deaths = &amp;quot;#e34a33&amp;quot;, Injuries = &amp;quot;#fdbb84&amp;quot;), 
                          domain = c(&amp;quot;Incidents with deaths&amp;quot;, &amp;quot;No deaths&amp;quot;))


leaflet(data = subset(dt, year == &amp;quot;2018&amp;quot;)) %&amp;gt;%
  addTiles() %&amp;gt;% 
  addCircleMarkers(lng = ~lon,
                         lat = ~lat,
                         popup = ~popup,
                         label = ~Location,
                         color = ifelse(dt$Deaths &amp;gt; 0, &amp;quot;#e34a33&amp;quot;,
                                        &amp;quot;#fdbb84&amp;quot;),
                         opacity = 0.3,
                         fillOpacity = 0.3,
                         radius = sqrt(dt$Deaths + dt$Injuries) + 6
        ) %&amp;gt;% 
  addLegend(position = &amp;quot;topright&amp;quot;, 
            pal = leafletColors, 
            values = c(&amp;quot;Incidents with deaths&amp;quot;, &amp;quot;No deaths&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;leaflet html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;options&#34;:{&#34;crs&#34;:{&#34;crsClass&#34;:&#34;L.CRS.EPSG3857&#34;,&#34;code&#34;:null,&#34;proj4def&#34;:null,&#34;projectedBounds&#34;:null,&#34;options&#34;:{}}},&#34;calls&#34;:[{&#34;method&#34;:&#34;addTiles&#34;,&#34;args&#34;:[&#34;//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&#34;,null,null,{&#34;minZoom&#34;:0,&#34;maxZoom&#34;:18,&#34;tileSize&#34;:256,&#34;subdomains&#34;:&#34;abc&#34;,&#34;errorTileUrl&#34;:&#34;&#34;,&#34;tms&#34;:false,&#34;noWrap&#34;:false,&#34;zoomOffset&#34;:0,&#34;zoomReverse&#34;:false,&#34;opacity&#34;:1,&#34;zIndex&#34;:1,&#34;detectRetina&#34;:false,&#34;attribution&#34;:&#34;&amp;copy; &lt;a href=\&#34;http://openstreetmap.org\&#34;&gt;OpenStreetMap&lt;\/a&gt; contributors, &lt;a href=\&#34;http://creativecommons.org/licenses/by-sa/2.0/\&#34;&gt;CC-BY-SA&lt;\/a&gt;&#34;}]},{&#34;method&#34;:&#34;addCircleMarkers&#34;,&#34;args&#34;:[[43.2624685,33.7556593,36.0998596,32.1840381,29.9510658,36.8573767,30.6953657,39.9525839,34.0522342,38.8033178,26.3107774,43.5978075,33.5206608,38.0405837,33.5206608,38.2616086,29.1871986,34.5794343,41.8389213,29.3780129,33.5215013,40.0455917],[-93.6371937,-96.536658,-80.244216,-96.8847194,-90.0715323,-88.4016041,-88.0398912,-75.1652215,-118.2436849,-76.9897278,-80.2532249,-84.7675139,-86.80249,-84.5037164,-86.80249,-76.4967919,-82.1400923,-118.1164613,-89.4795478,-95.1057625,-84.3538128,-86.0085955],[7,7,7.41421356237309,7,7,7,7.41421356237309,7,7,7.73205080756888,7,7,7,7,7,7,7,7,6,6,7,7.73205080756888,6,7.41421356237309,6,6,7,6,7,7,9.74165738677394,null,7.41421356237309,8.23606797749979,7.41421356237309,null,7,7.73205080756888,7,7.73205080756888,7.41421356237309,7,7,7,6,7.41421356237309,7,7,6,7,7.41421356237309,6,null,7,7,7,7,7,7.41421356237309,7,null,7,7,7,7,7,6,7,7.41421356237309,7,7,6,7,7,7,7,7.41421356237309,7,7,7,7,7,7,7.73205080756888,7.41421356237309,7,7.41421356237309,7,8.64575131106459,7,7,7,7.41421356237309,7,7,7,7,7,7.41421356237309,7,7,7,7,7,7,7.73205080756888,7,7.73205080756888,7,7,7,7,7,7.73205080756888,7,7.41421356237309,7,7.41421356237309,7,7,12.9282032302755,7.41421356237309,8.64575131106459,7.41421356237309,7,11.4772255750517,7,7.41421356237309,7.41421356237309,7,7,7,7.41421356237309,7.73205080756888,9.60555127546399,9.74165738677394,7.41421356237309,7,8.44948974278318,7,7.41421356237309,7,7,7,9.74165738677394,7.41421356237309,7,8.44948974278318,8.23606797749979,9,7,7,7.41421356237309,7,7,7.41421356237309,7,7,8,7,9.3166247903554,7,7,7,7,7,7.41421356237309,7.41421356237309,7.41421356237309,7.41421356237309,7.73205080756888,7,7.41421356237309,7.73205080756888,7,9.74165738677394,7,7.41421356237309,7,7,8,8.44948974278318,7.73205080756888,7,7.73205080756888,7,7,7.73205080756888,14.7177978870813,8,7.41421356237309,7.41421356237309,7.73205080756888,7,7,7.73205080756888,8.44948974278318,7,9.3166247903554,7,7.41421356237309,12.164414002969,6,7.41421356237309,7,7.73205080756888,7,7,7,8.64575131106459,7,7,7,7.73205080756888,7.41421356237309,7,9.74165738677394,8.64575131106459,7.73205080756888,9.3166247903554,7,7.41421356237309,7,7,7,7,8.23606797749979,7,7,7,7,7,7.41421356237309,7,7,7.41421356237309,7,8.23606797749979,7,7.41421356237309,7,7.73205080756888,7.73205080756888,7,8,7.41421356237309,7.73205080756888,7.73205080756888,7.73205080756888,7,7,8,9.16227766016838,7,8.82842712474619,7.41421356237309,8,9.87298334620742,8,7,11.1961524227066,7.41421356237309,7,7.41421356237309,6,12,8.44948974278318,7,7,8.44948974278318,7,7,7.41421356237309,7.41421356237309,7.41421356237309,7,9.87298334620742,7,7,7.41421356237309,8.23606797749979,7,7.41421356237309,8.44948974278318,7,8.23606797749979,7,8,7,8,7.41421356237309,7.73205080756888,7.41421356237309,7,7,8,7,7,10.1231056256177,7,7.73205080756888,7,7,7.41421356237309,8.23606797749979,7.73205080756888,7.73205080756888,8.23606797749979,7.41421356237309,7,9,6,7,7,13.4833147735479,8.23606797749979,7,7,7.73205080756888,7,7,11.1961524227066,7,7,6,8,7.73205080756888,7,8.23606797749979,7.73205080756888,7,7,7.73205080756888,7,7,7,8.44948974278318,7,7.41421356237309,7.41421356237309,7,7.41421356237309,7.41421356237309,7,7.41421356237309,7,7,8,7,8.44948974278318,7,7,7.41421356237309,7.41421356237309,7,8.44948974278318,7.41421356237309,9.16227766016838,7.41421356237309,7,6,7,7,7,8,11.4772255750517,7,7.41421356237309,7,7.41421356237309,7.73205080756888,7,7.73205080756888,6,7.41421356237309,7,7.41421356237309,7.73205080756888,7,6,9.16227766016838,6,7.73205080756888,7,7.41421356237309,8,7,7.41421356237309,7.73205080756888,7,7.41421356237309,7,7,7,7.73205080756888,7.41421356237309,7,7,7,7,7,7,7,7,7,6,7,7,7,6,6,7,7,7,7,7,8,7.73205080756888,7,7,7,7,7,7,8.44948974278318,8,7.41421356237309,7,8,7.73205080756888,7.41421356237309,7.41421356237309,7,7.73205080756888,7,7,7,6,7.41421356237309,8.64575131106459,7,7.73205080756888,7.41421356237309,7,10.3588989435407,8,7.41421356237309,8,7.41421356237309,7,7,6,8,7.41421356237309,8,7.73205080756888,7.41421356237309,8,7.73205080756888,8,7,7.41421356237309,8,7,6,7,7,7,8,7.41421356237309,8,7,10.8989794855664,7.73205080756888,6,6,7,7,7,10.4721359549996,6,7,8.23606797749979,7,11.8309518948453,7.41421356237309,7.73205080756888,7,7.73205080756888,7.73205080756888,7,7,7,10.4721359549996,8,7.41421356237309],null,null,{&#34;interactive&#34;:true,&#34;className&#34;:&#34;&#34;,&#34;stroke&#34;:true,&#34;color&#34;:[&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;],&#34;weight&#34;:5,&#34;opacity&#34;:0.3,&#34;fill&#34;:true,&#34;fillColor&#34;:[&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#fdbb84&#34;,&#34;#e34a33&#34;,&#34;#e34a33&#34;,&#34;#fdbb84&#34;],&#34;fillOpacity&#34;:0.3},null,null,[&#34;&lt;b&gt;Date: 2018-01-09&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 0&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 0&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;A man shot a pellet gun at a school bus full of children, shattering a window. No one was injured.[551]&#34;,&#34;&lt;b&gt;Date: 2018-01-10&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 0&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 0&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;A criminal justice club student picked up a loaded gun, belonging to an advisor, which the student thought was an unloaded training weapon. She then shot at a wall target, unintentionally firing a bullet, which went through the wall and broke a window. The advisor was a licensed peace officer permitted to carry a firearm on campus.[552]&#34;,&#34;&lt;b&gt;Date: 2018-01-20&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 1&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 0&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;A student was fatally shot at a party at 1 a.m. on the campus of Wake Forest University.[551]&#34;,&#34;&lt;b&gt;Date: 2018-01-22&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 0&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 1&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;A 16-year-old male student fired at a 15-year-old female classmate that he had briefly dated in the cafeteria of Italy High School. The gunman left the school immediately after opening fire and was arrested.[553][554][555]&#34;,&#34;&lt;b&gt;Date: 2018-01-22&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 0&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 1&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;Shots were fired from a truck in the parking lot of NET Charter High School, targeting a crowd of students during lunch time. One student was slightly injured, apparently from injuries unrelated to gunfire. One person was arrested in connection with the shooting.[551]&#34;,&#34;&lt;b&gt;Date: 2018-01-23&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 2&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 18&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;Marshall County High School shooting: Gabriel Ross Parker,[556] a 15-year-old male student shot 16 people in the lobby at Marshall County High School and caused non-gunshot injuries to 4 others. Two 15-year-old students died: one killed at the scene, another died of wounds at Vanderbilt Medical Center.[557][558][559]&#34;,&#34;&lt;b&gt;Date: 2018-01-25&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 0&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 0&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;A disagreement between two 16-year-old students escalated when Jonah Neal pulled out a handgun. School administrators noticed the weapon and tried to calm him; Neal then fled across campus. He was pursued by staff until he fired the gun four or five times into the air. Nobody was injured. Neal was taken into custody and charged for multiple offenses, including possession of a weapon on school property.[560]&#34;,&#34;&lt;b&gt;Date: 2018-01-31&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 1&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 0&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;A fight began in the school parking lot outside a basketball game, and the school went into lockdown after shots were fired. A 32-year-old male was transported by private vehicle to Nazareth Hospital with two gunshot wounds in his leg. He was transferred by helicopter to another hospital, but died from his injuries. Police announced that they are searching for an adult male suspect.[561]&#34;,&#34;&lt;b&gt;Date: 2018-02-01&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 0&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 5&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;Two 15-year-old students, a boy and a girl, were shot and injured inside a classroom at Sal Castro Middle School, which shares a campus with Belmont High School. Three other people suffered injuries unrelated to gunfire. A 12-year-old girl was arrested and charged with negligent discharge of a firearm.[562]&#34;,&#34;&lt;b&gt;Date: 2018-02-05&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 0&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 1&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;A student was taken to hospital after exiting Oxon Hill High School and going to speak to individuals in a vehicle who then attempted to rob, and subsequently shot and wounded him in the school&#39;s parking lot. Two other students were arrested and charged with attempted murder and robbery.[563]&#34;,&#34;&lt;b&gt;Date: 2018-02-14&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 17&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 17&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;Marjory Stoneman Douglas High School shooting: A 19-year-old former student whose behavior had led to his expulsion allegedly began shooting students and staff members with a semi-automatic rifle at Marjory Stoneman Douglas High School after activating a fire alarm. Seventeen people were killed, and 17 others were injured. The suspected shooter blended in with the crowd of fleeing students and was arrested in a residential area of neighboring Coral Springs after walking away from the school.[564] He was later charged with murder and attempted murder.[565]&#34;,&#34;&lt;b&gt;Date: 2018-03-02&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 2&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 0&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;Central Michigan University shooting: 19-year-old student James Eric Davis Jr. shot and killed his mother and father when they came to campus to take him home for spring break. After the shooting Davis fled and the campus was placed on lockdown. Around 15 hours later police arrested him and took him to a local hospital. The incident disrupted the travel plans of students and campus activities for several days. Davis was charged with two counts of murder and one count of unlawful possession of a firearm used to commit murder.[566][567][568]&#34;,&#34;&lt;b&gt;Date: 2018-03-07&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 1&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 2&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;Huffman High School: One student was killed and another injured when shots were fired in the school building, prompting the school to go into lockdown shortly after the bell rang for school dismissal. Law enforcement originally labeled the shooting as \&#34;accidental\&#34;.[569] Subsequently, a 17-year-old male student was charged with manslaughter and will be charged as \&#34;being a certain person forbidden to possess a pistol\&#34;.[570] Although the school has metal detectors, they were not being used that day.[571] School resource officers were onsite at the time of the shooting.[570]&#34;,&#34;&lt;b&gt;Date: 2018-03-09&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 0&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 1&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;Frederick Douglass High School: A 16-year-old male student unintentionally shot himself with a gun he had smuggled into a classroom, sustaining an injury to his left thumb. Police announced they were charging him with wanton endangerment and possession of a weapon on school property. After the shooting, the school superintendent announced that stationary metal detectors would be installed and all students would be required to enter through them.[572]&#34;,&#34;&lt;b&gt;Date: 2018-03-14&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 2&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 1&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;University of Alabama at Birmingham: A disgruntled employee entered the UAB Highlands Hospital on the campus of the University of Alabama at Birmingham and shot two hospital employees, fatally wounding one, on the second floor. The perpetrator then shot and fatally wounded himself.[573]&#34;,&#34;&lt;b&gt;Date: 2018-03-20&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 2&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 1&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;Great Mills High School: School placed on lockdown after a shooting occurred in the morning. A 17-year-old male student, armed with a handgun, shot and fatally injured a female student (with whom he had a prior relationship) and wounded a male student. The student shot himself in the head, fatally, while the school resource officer simultaneously shot at him.\n[574]&#34;,&#34;&lt;b&gt;Date: 2018-04-20&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 0&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 1&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;Forest High School: A student was shot and injured in a classroom shortly before a national school walkout for gun control measures. His injuries were not life-threatening. A 19-year-old former student was arrested.[citation needed]&#34;,&#34;&lt;b&gt;Date: 2018-05-11&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 0&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 1&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;Highland High School: A 14-year-old former student allegedly fired a semi-automatic rifle shortly before classes were to begin. A 15-year-old was struck in the shoulder but went into surgery and was expected to recover fully. The suspect ditched the gun in a field and was arrested nearby, and faces a charge of attempted murder.[575]&#34;,&#34;&lt;b&gt;Date: 2018-05-16&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 0&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 1&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;Dixon High School: A 19-year-old student recently kicked off football team allegedly fired shots prior to graduation rehearsal. A school resource officer shot the suspect, who suffered non-life threatening injuries.[576][577]&#34;,&#34;&lt;b&gt;Date: 2018-05-18&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 10&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 10&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;Santa Fe High School shooting: School was evacuated when fire alarms pulled at 7:45 am after students said they had heard gunshots. Shooter had a shotgun and .38 revolver. Multiple IEDs and pipe bombs were also found around the school.[578]&#34;,&#34;&lt;b&gt;Date: 2018-05-18&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 1&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 3&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;Mount Zion High School: An argument led to a shooting in a parking lot after a high school graduation ceremony for graduates of Perry Learning Center. Mount Zion High School provided overflow parking for people attending the ceremony.[579]&#34;,&#34;&lt;b&gt;Date: 2018-05-25&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Deaths: 0&lt;\/b&gt;&lt;br/&gt;&lt;b&gt;Injuries: 2&lt;\/b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Description: &lt;\/b&gt;&lt;br/&gt;Noblesville West Middle School: Two people were shot and injured in a shooting at Noblesville West Middle School in Noblesville, Indiana. The shooter is an unnamed middle school student who exited his science classroom and entered with a gun. When he returned, he shot a female student and the science teacher, identified as Jason Seamen. Seamen was shot in the abdomen, hip, and forearm. The gun was wrestled away by Seamen after both he and the female student were shot.[580][581]&#34;],null,[&#34;Forest City, Iowa&#34;,&#34;Denison, Texas&#34;,&#34;Winston-Salem, North Carolina&#34;,&#34;Italy, Texas&#34;,&#34;New Orleans, Louisiana&#34;,&#34;Marshall County, Kentucky&#34;,&#34;Mobile, Alabama&#34;,&#34;Philadelphia, Pennsylvania&#34;,&#34;Los Angeles, California&#34;,&#34;Oxon Hill, Maryland&#34;,&#34;Parkland, Florida&#34;,&#34;Mount Pleasant, Michigan&#34;,&#34;Birmingham, Alabama&#34;,&#34;Lexington, Kentucky&#34;,&#34;Birmingham, Alabama&#34;,&#34;Great Mills, Maryland&#34;,&#34;Ocala, Florida&#34;,&#34;Palmdale, California&#34;,&#34;Dixon, Illinois&#34;,&#34;Santa Fe, Texas&#34;,&#34;Jonesboro, Georgia&#34;,&#34;Noblesville, Indiana&#34;],{&#34;interactive&#34;:false,&#34;permanent&#34;:false,&#34;direction&#34;:&#34;auto&#34;,&#34;opacity&#34;:1,&#34;offset&#34;:[0,0],&#34;textsize&#34;:&#34;10px&#34;,&#34;textOnly&#34;:false,&#34;className&#34;:&#34;&#34;,&#34;sticky&#34;:true},null]},{&#34;method&#34;:&#34;addLegend&#34;,&#34;args&#34;:[{&#34;colors&#34;:[&#34;#E34A33&#34;,&#34;#FDBB84&#34;],&#34;labels&#34;:[&#34;Incidents with deaths&#34;,&#34;No deaths&#34;],&#34;na_color&#34;:null,&#34;na_label&#34;:&#34;NA&#34;,&#34;opacity&#34;:0.5,&#34;position&#34;:&#34;topright&#34;,&#34;type&#34;:&#34;factor&#34;,&#34;title&#34;:null,&#34;extra&#34;:null,&#34;layerId&#34;:null,&#34;className&#34;:&#34;info legend&#34;,&#34;group&#34;:null}]}],&#34;limits&#34;:{&#34;lat&#34;:[26.3107774,43.5978075],&#34;lng&#34;:[-118.2436849,-75.1652215]}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;state-statistics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;State Statistics&lt;/h1&gt;
&lt;p&gt;As in the dashboard we are going to plot to summary plots for a absolute count and share of death and injured people per state (here only year 2018 again).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; myFillColors &amp;lt;- c(Deaths = &amp;quot;#e34a33&amp;quot;, Injuries = &amp;quot;#fdbb84&amp;quot;)

dt %&amp;gt;%
  filter(year == &amp;quot;2018&amp;quot;) %&amp;gt;% 
  group_by(State) %&amp;gt;% 
  summarise(Deaths = sum(Deaths, na.rm = T),
            Injuries = sum(Injuries, na.rm = T),
            Total = sum(Deaths, na.rm = T) + sum(Injuries, na.rm = T)) %&amp;gt;% 
  gather(key = category, value = count, Deaths, Injuries) %&amp;gt;% 
  ggplot() +
  geom_col(aes(x = reorder(State, Total), y = count, fill = category),
           alpha = 0.7, width = 0.8) +
  scale_fill_manual(values = myFillColors,
                    guide = guide_legend(title = NULL, keywidth = 1, keyheight = 1)) +
  xlab(&amp;quot;State&amp;quot;) +
  ylab(&amp;quot;Injured and Death People&amp;quot;) +
  coord_flip() +
  theme_minimal() +
  theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-27-usa-school-shootings_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dt %&amp;gt;%
  group_by(State) %&amp;gt;% 
  filter(year == &amp;quot;2018&amp;quot;) %&amp;gt;% 
  summarise(Deaths = sum(Deaths, na.rm = T),
            Injuries = sum(Injuries, na.rm = T),
            Total = sum(Deaths, na.rm = T) + sum(Injuries, na.rm = T)) %&amp;gt;% 
  gather(key = category, value = count, Deaths, Injuries) %&amp;gt;% 
  ggplot() +
  geom_col(aes(x = reorder(State, Total), y = count, fill = category),
           alpha = 0.7, width = 0.8, position = &amp;quot;fill&amp;quot;) +
  scale_fill_manual(values = myFillColors,
                    guide = guide_legend(title = NULL, keywidth = 1, keyheight = 1)) +
  scale_y_continuous(labels = scales::percent) +
  xlab(&amp;quot;State&amp;quot;) +
  ylab(&amp;quot;Share&amp;quot;) +
  coord_flip() +
  theme_minimal() +
  theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 2 rows containing missing values (geom_col).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-27-usa-school-shootings_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>San Francisco Crime Heatmaps in R Using BigQuery bigrquery and ggmap</title>
      <link>/post/san-francisco-crime-heatmaps-in-r-using-bigquery-bigrquery-and-ggmap/</link>
      <pubDate>Sun, 27 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/san-francisco-crime-heatmaps-in-r-using-bigquery-bigrquery-and-ggmap/</guid>
      <description>&lt;p&gt;The aim of today’s blog post is to give a short introduction into the usage of &lt;a href=&#34;https://bigquery.cloud.google.com&#34;&gt;BigQuery&lt;/a&gt; inside of R. BigQuery is a product within the &lt;a href=&#34;https://cloud.google.com/?hl=de&#34;&gt;Google Cloud Platform&lt;/a&gt; and serves as a data warehouse for storage and analytics at high scale. It supports standard SQL dialect which makes it easy to use for people with SQL experience.&lt;/p&gt;
&lt;p&gt;We are going to use one of the public data sets available on BigQuery and have a short look at the recorded incidents of the San Francisco Police Departments. Further, we are going to plot the data on a map to see areas with high crime densities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What you are going to learn in this blogpost:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to use the &lt;code&gt;bigrquery&lt;/code&gt; package&lt;/li&gt;
&lt;li&gt;How to plot data on a map using the &lt;code&gt;ggmap&lt;/code&gt; package&lt;/li&gt;
&lt;li&gt;How to use the Google Maps Geocoding API to convert addresses to latitude / longitude data.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;installation-and-environment-setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Installation and Environment Setup&lt;/h1&gt;
&lt;p&gt;First things first: There is a package called &lt;code&gt;bigrquery&lt;/code&gt; which can be used to query BigQuery tables from R. You can find some quick start information about the &lt;code&gt;bigrquery&lt;/code&gt; package on GitHub at &lt;a href=&#34;https://github.com/r-dbi/bigrquery&#34; class=&#34;uri&#34;&gt;https://github.com/r-dbi/bigrquery&lt;/a&gt;. The package is also available on CRAN so you can install it using the &lt;code&gt;install.packages()&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if (!require(&amp;quot;bigrquery&amp;quot;, quietly = T)) {
  install.packages(&amp;quot;bigrquery&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also need to install the newest &lt;code&gt;ggmap&lt;/code&gt; version from GitHub, since we need at least version 2.7 to support the Google Maps API for our geocoding of addresses later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# If you need to install ggmap just uncomment the line below
#devtools::install_github(&amp;quot;dkahle/ggmap&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After installing the packages we load them together with the &lt;code&gt;tidyverse&lt;/code&gt; package which we are going to use, too.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;bigrquery&amp;quot;, quietly  = T, warn.conflicts = F)
library(&amp;quot;tidyverse&amp;quot;, quietly  = T, warn.conflicts = F)
library(&amp;quot;ggmap&amp;quot;, quietly = T, warn.conflicts = F)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;google-cloud-platform-setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Google Cloud Platform Setup&lt;/h1&gt;
&lt;p&gt;In order to be able to use BigQuery you need to have a Google account. Further, you need to connect your account with Google Cloud Platform (GCP) and create a project there. This is essential for BigQuery because it will need a billing project. All GCP products need to be assigned to a certain project since Google’s billing for the service usage is project based.&lt;/p&gt;
&lt;p&gt;Nevertheless, GCP will not charge you for every single service use. There are a lot of services which provide you with some free usage limit per month. Further, if you connect your account with GCP the first time, they give you a free 300$ credit for 12 a months period. You can use this credit for any product on GCP.&lt;/p&gt;
&lt;p&gt;The following page will give you access to the free trial and provide you with some additional information about the products and their monthly free usage limits: &lt;a href=&#34;https://cloud.google.com/free/&#34; class=&#34;uri&#34;&gt;https://cloud.google.com/free/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After you have created your GCP project, we can start with using BigQuery.&lt;/p&gt;
&lt;div id=&#34;bigquery-and-the-bigrquery-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;BigQuery and the &lt;code&gt;bigrquery&lt;/code&gt; Package&lt;/h2&gt;
&lt;p&gt;As written in the GitHub page &lt;a href=&#34;https://github.com/r-dbi/bigrquery&#34;&gt;README&lt;/a&gt; there a basically three possibilities to use the &lt;code&gt;bigrquery&lt;/code&gt; package:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The low-level API (BigQuery REST API)&lt;/li&gt;
&lt;li&gt;The DBI Interface (for &lt;code&gt;DBI&lt;/code&gt; library like interactions)&lt;/li&gt;
&lt;li&gt;The dplyr Interface&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I am only going to show you the first method but you can read about the others in the README. To get started we need to provide a project-id for billing purpose:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;billing &amp;lt;-  project_id # insert your gcp project-id here&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is basically all what we needed to do in order to get everything set up for our work with BigQuery.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;query-examples-using-the-san-francisco-police-department-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Query Examples Using the San Francisco Police Department Data&lt;/h1&gt;
&lt;p&gt;As mentioned above, we will use a public BigQuery table which provides data of all San Francisco police departments. You can have a look at the table by using the preview function on the &lt;a href=&#34;https://bigquery.cloud.google.com/table/bigquery-public-data:san_francisco.sfpd_incidents?tab=preview&#34;&gt;BigQuery table page&lt;/a&gt;. Google won’t charge you for the previews, but if you press the &lt;code&gt;Query Table&lt;/code&gt; button and run SQL queries on the table, you will get charged for each query depending on the data amount it needs to process. &lt;strong&gt;But don’t be afraid:&lt;/strong&gt; The first 1TB each month are for free, so we won’t have a problem running our analysis.&lt;/p&gt;
&lt;p&gt;We are going to run 3 queries to ask the following questions on the data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On which weekday did the most incidents happen?&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;On which weekday did most arrestments happen and is it the same day as in the first question?&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Where do we find the highest crime density within San Francisco? (Here we are going to use &lt;code&gt;ggmap&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;incidents-per-weekday&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Incidents per Weekday&lt;/h2&gt;
&lt;p&gt;To get the incidents per weekday, we run the following SQL command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sql &amp;lt;- &amp;quot;
SELECT 
  dayofweek,
  COUNT(*) AS incidents
FROM 
  `bigquery-public-data.san_francisco.sfpd_incidents`
GROUP BY 
  dayofweek
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We take this SQL command and use the &lt;code&gt;bq_project_query()&lt;/code&gt; function from the &lt;code&gt;bigrquery&lt;/code&gt; package. It will need a billing project variable for which we use our project-id (the &lt;code&gt;billing&lt;/code&gt; variable we created earlier). The function will save our results into a table in BigQuery (either temporary or provided by the user). Then we can use the &lt;code&gt;bq_table_download()&lt;/code&gt; function to download our table and use it within R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bq_dt &amp;lt;- bq_project_query(billing, sql)
dt &amp;lt;- bq_table_download(bq_dt, quiet = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we downloaded the data we can also plot it:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-20-san-francisco-crime-heatmaps-in-r-using-bigquery-bigrquery-and-ggmap_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The data show that &lt;strong&gt;most incidents happened on Fridays&lt;/strong&gt;. The proportion of incidents on Fridays was above average, where else the fewest incidents happened on Sundays. Does that really mean that also the worse crime incidents used to happen on Fridays or is this high proportion due to minor crimes committed by people going out? We can use the &lt;em&gt;resolution&lt;/em&gt; column and just look for incidents with arrestment. So our next question will be:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;On which weekday did most arrestments happen?&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-arrestings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Most Arrestings&lt;/h2&gt;
&lt;p&gt;To answer the question we need to adjust our SQL query from above. We use the &lt;code&gt;bq_project_query()&lt;/code&gt; and &lt;code&gt;bq_table_download()&lt;/code&gt; command again to get the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sql &amp;lt;- &amp;#39;
SELECT 
  dayofweek,
  COUNT(*) AS incidents
FROM 
  `bigquery-public-data.san_francisco.sfpd_incidents`
WHERE 
  resolution LIKE &amp;quot;%ARREST%&amp;quot;
GROUP BY 
  dayofweek
&amp;#39;

bq_dt &amp;lt;- bq_project_query(billing, sql)
dt2 &amp;lt;- bq_table_download(bq_dt, quiet = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-20-san-francisco-crime-heatmaps-in-r-using-bigquery-bigrquery-and-ggmap_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Most of the incidents with arrestment accured on Wednesdays&lt;/strong&gt;. Although Fridays counted most incidents in the plot before, the proportion for incidents with arrestment on Fridays is just average. Furthermore, it’s interesting that the proportion on other weekdays did not change much.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;san-francisco-2017-crime-map-using-the-ggmap-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;San Francisco 2017 Crime Map Using the &lt;code&gt;ggmap&lt;/code&gt; Package&lt;/h2&gt;
&lt;p&gt;The following information is cited from the &lt;a href=&#34;https://en.wikipedia.org/wiki/San_Francisco_Police_Department#Stations&#34;&gt;Wikipedia article&lt;/a&gt; (as of Mai 2018):&lt;/p&gt;
&lt;p&gt;“The SFPD currently has 10 main police stations throughout the city in addition to a number of police substations.&lt;/p&gt;
&lt;p&gt;Metro Division:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Central Station: 766 Vallejo St. San Francisco, CA 94133&lt;/li&gt;
&lt;li&gt;Mission Station: 630 Valencia St. San Francisco, CA 94110&lt;/li&gt;
&lt;li&gt;Northern Station: 1125 Fillmore St. San Francisco, CA 94115&lt;/li&gt;
&lt;li&gt;Southern Station, Public Safety Building: 1251 3rd St. San Francisco, CA 94103&lt;/li&gt;
&lt;li&gt;Tenderloin Station: 301 Eddy St. San Francisco, CA 94102&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Golden Gate Division:&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Bayview Station: 201 Williams Ave. San Francisco, CA 94124&lt;/li&gt;
&lt;li&gt;Ingleside Station: 1 Sgt. John V. Young Ln. San Francisco, CA 94112-2408&lt;/li&gt;
&lt;li&gt;Park Station: 1899 Waller Street San Francisco, CA 94117&lt;/li&gt;
&lt;li&gt;Richmond Station: 461 6th Ave San Francisco, CA 94118&lt;/li&gt;
&lt;li&gt;Taraval Station: 2345 24th Ave. San Francisco, CA 94116&amp;quot;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We are going transform the given addresses into latitude and longitude data to plot them on our heat map later. To do so, we will use the &lt;strong&gt;Google Maps Geocoding API&lt;/strong&gt; with the &lt;code&gt;geocode&lt;/code&gt; function of the &lt;code&gt;ggmap&lt;/code&gt; package. &lt;code&gt;geocode&lt;/code&gt; takes addresses and provides you with latitude / longitude data by querying the Google API (you could also use the Data Science Toolkit as resource). The function can be used with a public API-key but then you might get in trouble with the limits of the public key because it only allows a certain limit of requests.&lt;/p&gt;
&lt;p&gt;Since you already created a GCP project, it is easy to get your own API-key for Google Maps Geocoding API. Just visit the &lt;a href=&#34;https://developers.google.com/maps/documentation/geocoding/start?hl=en#get-a-key&#34;&gt;developer documentation&lt;/a&gt; and follow the instructions. It will tell you how to active the API in your GCP project. After that, Google will provide you with a key which you can use in the code chunk below. I saved my key as a .txt file within this R-Project(&lt;code&gt;./additional_data/api-key.txt&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;api_key &amp;lt;- read_lines(&amp;quot;./additional_data/api-key.txt&amp;quot;) #you would need to insert your api key here

addresses &amp;lt;- c(Central = &amp;quot;766 Vallejo St. San Francisco&amp;quot;,
              Mission =  &amp;quot;630 Valencia St. San Francisco&amp;quot;,
              Northern = &amp;quot;1125 Fillmore St. San Francisco&amp;quot;,
              Southern = &amp;quot;1251 3rd St. San Francisco&amp;quot;,
              Tenderloin = &amp;quot;301 Eddy St. San Francisco&amp;quot;,
              Bayview = &amp;quot;201 Williams Ave. San Francisco&amp;quot;,
              Ingleside = &amp;quot;1 Sgt. John V. Young Ln. San Francisco&amp;quot;,
              Park = &amp;quot;1899 Waller Street San Francisco&amp;quot;,
              Richmond = &amp;quot;461 6th Ave San Francisco&amp;quot;,
              Taraval = &amp;quot;2345 24th Ave. San Francisco&amp;quot;)

register_google(api_key)
pd_location &amp;lt;- geocode(addresses, messaging = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check what &lt;code&gt;geocode&lt;/code&gt; gave us:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pd_location&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           .id       lon      lat
## 1     Central -122.4100 37.79872
## 2     Mission -122.4220 37.76285
## 3    Northern -122.4325 37.78022
## 4    Southern -122.3894 37.77238
## 5  Tenderloin -122.4130 37.78367
## 6     Bayview -122.3980 37.72976
## 7   Ingleside -122.4463 37.72468
## 8        Park -122.4553 37.76780
## 9    Richmond -122.4645 37.78001
## 10    Taraval -122.4815 37.74371&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you see, it gave us a table with a id (name of our police department) and the corresponding lat / lon data. We ware going to use this data to add the police departments to our map later.&lt;/p&gt;
&lt;div id=&#34;getting-crime-incidents-and-their-location&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Getting Crime Incidents and their Location&lt;/h3&gt;
&lt;p&gt;To get the crime data for our map we make another query on the &lt;code&gt;san_francisco.sfpd_incidents&lt;/code&gt; table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sql &amp;lt;- &amp;quot;
SELECT 
  location,
  timestamp,
  dayofweek

FROM 
  `bigquery-public-data.san_francisco.sfpd_incidents`
&amp;quot;
bq_dt &amp;lt;- bq_project_query(billing, sql)
dt4 &amp;lt;- bq_table_download(bq_dt, quiet = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now need extract latitude / longitude data from the location column and filter it since there is a outlier in the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dt4 &amp;lt;- dt4 %&amp;gt;% 
  mutate(location = gsub(&amp;quot;\\(|\\)| &amp;quot;,&amp;quot;&amp;quot;,location)) %&amp;gt;% 
  separate(location, sep = &amp;quot;,&amp;quot;, into = c(&amp;quot;lat&amp;quot;,&amp;quot;long&amp;quot;)) %&amp;gt;% 
  mutate(lon = as.numeric(long),
         lat = as.numeric(lat),
         year = format(timestamp, &amp;quot;%Y&amp;quot;)) %&amp;gt;% 
  filter(lat &amp;lt; 90)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can get our hands dirty and start creating the heat map plot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-the-heatmap-for-2017&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating the Heatmap for 2017&lt;/h3&gt;
&lt;p&gt;What we need to do first is creating a area-box for our map. This can be done using the &lt;code&gt;make_bbox()&lt;/code&gt; function. This area can be used to download a plot map using the &lt;code&gt;get_map()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;area &amp;lt;- make_bbox(lon = lon, lat = lat, data = dt4)
area&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       left     bottom      right        top 
## -122.52109   37.70224 -122.35731   37.82626&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;area_map &amp;lt;- get_map(area,  source = &amp;quot;stamen&amp;quot;)
ggmap(area_map, extend = &amp;quot;device&amp;quot;, darken = 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-20-san-francisco-crime-heatmaps-in-r-using-bigquery-bigrquery-and-ggmap_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Perfect! We got our map and used the &lt;code&gt;ggmap()&lt;/code&gt; function to plot it. Now we can just add ggplot layers as known from &lt;code&gt;ggplot&lt;/code&gt;. In our example we use the &lt;code&gt;stat_density2d()&lt;/code&gt; layer to create a heat map with 2017s data. We will also add labels for the position of the Police Departments:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggmap(area_map, extend = &amp;quot;device&amp;quot;, darken = 0.3) +
  stat_density2d(aes(x = lon, y = lat, fill = ..level../10), alpha = 0.2,
                 data = subset(dt4, year == &amp;quot;2017&amp;quot;), geom = &amp;quot;polygon&amp;quot;, bins = 30) +
  geom_label(aes(x = lon, y = lat, label = .id), data = pd_location, size = 3, alpha = 0.5) +
  scale_fill_gradient2(low = &amp;quot;yellow&amp;quot;, mid = &amp;quot;orange&amp;quot;, high = &amp;quot;firebrick2&amp;quot;, midpoint = 80,
                       guide = guide_colorbar(title = &amp;quot;Level&amp;quot;)) +
  geom_text(aes(x = -122.41, y = 37.82, label = &amp;quot;San Francisco Crimes 2017&amp;quot;), size = 6,
            color = &amp;quot;white&amp;quot;) +
  theme_void() +
  theme(legend.position = c(0.15,0.93),
        legend.direction = &amp;quot;horizontal&amp;quot;,
        legend.background = element_rect(fill = alpha(&amp;quot;white&amp;quot;, 0.8), 
                                  size = 0.5, linetype = &amp;quot;solid&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-20-san-francisco-crime-heatmaps-in-r-using-bigquery-bigrquery-and-ggmap_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another possibility would be to use &lt;code&gt;geom_point()&lt;/code&gt; to plot every single crime instance and use the alpha parameter to adjust for a correct density representation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggmap(area_map, extend = &amp;quot;device&amp;quot;, darken = 0.3) +
  geom_point(aes(x = lon, y = lat), alpha = 0.02, size = 1.5, color = &amp;quot;orange&amp;quot;,
                 data = subset(dt4, year == &amp;quot;2017&amp;quot;)) +
  geom_label(aes(x = lon, y = lat, label = .id), data = pd_location, size = 3, alpha = 0.7) +
  theme_nothing() +
  geom_text(aes(x = -122.41, y = 37.82, label = &amp;quot;San Francisco Crimes 2017&amp;quot;), size = 6,
            color = &amp;quot;white&amp;quot;) +
  theme(legend.position = c(0.2,0.93),
        legend.direction = &amp;quot;horizontal&amp;quot;,
        legend.background = element_rect(fill = alpha(&amp;quot;white&amp;quot;, 0.8), 
                                  size = 0.5, linetype = &amp;quot;solid&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-20-san-francisco-crime-heatmaps-in-r-using-bigquery-bigrquery-and-ggmap_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we see, there have been a lot of incidents around Tenderloin and Southern district. You better be careful when visiting that area.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion-and-further-ressources&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion and Further Ressources&lt;/h1&gt;
&lt;p&gt;We went through a short introduction into &lt;code&gt;bigrquery&lt;/code&gt; and the &lt;code&gt;ggmap&lt;/code&gt; package. We learned how to query data from BigQuery and how to plot that data on a map. Further, we used the Google Maps Geocoding API to transform addresses into latitude and longitude data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;At this point I want to provide a summary of useful ressources:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BigQuery:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://console.cloud.google.com/getting-started&#34;&gt;Google Cloud Platform getting started page&lt;/a&gt; to create your first GCP project&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/bigquery/&#34;&gt;Official Big Query page&lt;/a&gt; for information about the product as well as documentation&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/bigquery/sample-tables&#34;&gt;BigQuery sample tables&lt;/a&gt; which you can use to play around with&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://bigquery.cloud.google.com/&#34;&gt;BigQuery web user interface&lt;/a&gt; where you can make some queries after having created a project&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r-dbi/bigrquery&#34;&gt;&lt;code&gt;Bigrquery&lt;/code&gt; GitHub page&lt;/a&gt; for documentation and further usage examples&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Google Maps Geocoding API:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://console.developers.google.com/apis/library&#34;&gt;GCP API Library&lt;/a&gt; where you can activate the Geocoding API withing GCP&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://developers.google.com/maps/documentation/geocoding/get-api-key&#34;&gt;Geocoding Documentation - get a key&lt;/a&gt; - here you can follow the steps to request your personal API key&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;ggmap&lt;/code&gt; Package:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ggmap&lt;/code&gt; &lt;a href=&#34;https://github.com/dkahle/ggmap&#34;&gt;Github page&lt;/a&gt; - here you find more detailed information on the usage of &lt;code&gt;ggmap&lt;/code&gt; and the &lt;code&gt;geocode()&lt;/code&gt; function&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading this blog post. As always: If you have any questions, corrections, interesting additional information or just want to add something else, feel free to comment or contact me.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Oktoberfest Analysis Part II</title>
      <link>/post/oktoberfest-analysis-part-ii/</link>
      <pubDate>Wed, 16 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/oktoberfest-analysis-part-ii/</guid>
      <description>&lt;p&gt;Everybody knows the Oktoberfest which takes place in Munich every year. In this blog post series we are going to look into a public availabe dataset and try to gain some insights about the Oktoberfest.&lt;/p&gt;
&lt;p&gt;In the &lt;a href=&#34;https://somtom.github.io/post/oktoberfest-analysis-part-i/&#34;&gt;first part&lt;/a&gt; we loaded and described the data. We also analysed the price and consumption of beer and hendl (chicken) over the years.&lt;/p&gt;
&lt;p&gt;In this second part we are going to have a closer look at the Bavarian Central Agricultural Festival - also known as “ZLF”. We will look at at its influence on beer and hendl consumption, and on the the visitor count. After that we are going to analyse the influence of the 9/11 terror attacks on mean daily visitor count.&lt;/p&gt;
&lt;div id=&#34;aim&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Aim&lt;/h1&gt;
&lt;p&gt;Since I am currently diving into the field of data analysis and machine learning, I decided to start my first public analysis in order to use the tools I have been learning so far. The aim of this exploratory data analysis was to create some insights about the Oktoberfest using the public available Oktoberfest data set from the &lt;a href=&#34;https://www.opengov-muenchen.de/&#34;&gt;Munich Open Data side&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Further, I wanted to try the Munich Open Data API to export the data from the server. My biggest aim, though, is to improve my analysis skills by getting feedback from the community.&lt;br /&gt;
That is why I would really appreciate your feedback. Feel free to comment on this post and to contact me.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;methods-and-material&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Methods and Material&lt;/h1&gt;
&lt;p&gt;The data description, as well as the importing and processing steps, can be found in the &lt;a href=&#34;https://somtom.github.io/post/oktoberfest-analysis-part-i/&#34;&gt;first part&lt;/a&gt; of this series.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;data-analysis---part-ii&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Analysis - Part II&lt;/h1&gt;
&lt;div id=&#34;bavarian-central-agricultural-festival-zlf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bavarian Central Agricultural Festival (ZLF)&lt;/h2&gt;
&lt;p&gt;The ZLF is an agricultural exhibition which takes place right next to the Oktoberfest at the Theresienwiese. Before 1996 the exhibition was held every three years. From there on it has been taken place every 4 years.&lt;/p&gt;
&lt;p&gt;We are going to cover the questions whether or not the ZLF brought more visitors to the Oktoberfest or increased the beer and hendl consumption.&lt;/p&gt;
&lt;div id=&#34;did-the-zlf-bring-more-visitor-to-the-oktoberfest&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Did the ZLF Bring More Visitor to the Oktoberfest?&lt;/h3&gt;
&lt;p&gt;Since the ZLF is at the same location as the Oktoberfest a lot of farmers and other visitors who maybe would normally not come to the Oktoberfest are going there in order to see the exhibition. This could have an influence on the mean daily visitor count. We will further investigate this hypothesis by looking at our data. In order to adjust for the duration we will use the mean daily visitor count for our analysis:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-16-oktoberfest-analysis-part-ii_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the mean daily visitor count varies around 387 thousand. There was a huge drop in 2001 which could be due to the terror attacks at the World Trade Center which took place a few weeks before the start of the Oktoberfest. Two other big drops compared to the previous year can be found in 1988 and 2016. For 2016 one could also argue that this drop is due to the shooting in Munich on July 22nd in the same year and because of the general fear of terror attacks around that time. Actually my research did not find any reasons for the drop in 1988. Even the weather was good during the Oktoberfest in that year.&lt;/p&gt;
&lt;p&gt;Looking at years the ZLF took place, we can not tell whether the total visitor count is greater or less in general. It looks like, though, that the general mean daily visitor count has been varying around a lower level since 2001 (9/11 year). We are going to investigate this hypothesis later.&lt;/p&gt;
&lt;p&gt;But first, let’s have a closer look at the total visitor count distribution for years with and without the ZLF:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-16-oktoberfest-analysis-part-ii_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like both box plots are quiet similar. The median of ZLF years is sligthly higher. Nevertheless, we need to be careful, as we have fewer data points for ZLF years than we do for normal years.&lt;/p&gt;
&lt;p&gt;We will continue with testing the null hypothesis:&lt;br /&gt;
&lt;em&gt;“ZLF years do not have an impact on mean daily visitor count”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To do so we first start with a test for normal distribution in order to select a proper statistical test for comparison. We are going to use the Shapiro-Wilk test for that purpose. It tests the null hypothesis that &lt;em&gt;the data is normally distributed&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for normal distribution of ZLF years
shapiro.test(dt[dt$zlf == 1,]$visitors_day)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  dt[dt$zlf == 1, ]$visitors_day
## W = 0.9461, p-value = 0.6473&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for normal distribution of normal years
shapiro.test(dt[dt$zlf == 0,]$visitors_day)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  dt[dt$zlf == 0, ]$visitors_day
## W = 0.96197, p-value = 0.4793&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The outcome of the Shapiro-Wilk test suggests that both groups have normal distributed values. The quantile-quantile plot also supports this, except of a few outliers at the ends:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-16-oktoberfest-analysis-part-ii_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will assume that our data is normally distributed and continue with a t-test to compare the means of the two groups:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(visitors_total ~ zlf, data = dt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  visitors_total by zlf
## t = -0.50311, df = 12.655, p-value = 0.6235
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -456903.6  284681.4
## sample estimates:
## mean in group 0 mean in group 1 
##         6291667         6377778&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The outcome of our tests confirms the null hypothesis (&lt;em&gt;the difference in means is equal to 0&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;This means that, given this results, &lt;strong&gt;the mean daily Oktoberfest visitor count in ZLF years is not statistically different to normal years&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;did-the-visitors-consume-more-beer-in-zlf-years&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Did the Visitors Consume More Beer in ZLF Years?&lt;/h3&gt;
&lt;p&gt;Now that we know that the ZLF did not bring more visitors to the Oktoberfest, we could ask whether the visitors consumed more beer in ZLF years. We are going to take a part of the beer consumption plot from part 1 and color the ZLF years:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-16-oktoberfest-analysis-part-ii_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At a first view we cannot see any differences in years with and years without the ZLF. They both follow a similar development. We are going to use the same procedure as above to test for differences. But first a look at the box plots:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-16-oktoberfest-analysis-part-ii_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both box plots look similar again but the ZLF year data seems to be split. We now start with a test for normal distribution and continue with a proper statistical comparison test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for normal distribution of ZLF years
shapiro.test(dt[dt$zlf == 1,]$beer_cons_per_visitor)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  dt[dt$zlf == 1, ]$beer_cons_per_visitor
## W = 0.87501, p-value = 0.139&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for normal distribution of normal years
shapiro.test(dt[dt$zlf == 0,]$beer_cons_per_visitor)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  dt[dt$zlf == 0, ]$beer_cons_per_visitor
## W = 0.92379, p-value = 0.07088&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The outcome of the Shapiro-Wilk test again suggests that both groups have normal distributed values. Nevertheless the p-value is way smaller than before.&lt;/p&gt;
&lt;p&gt;The quantile-quantile plot shows us why: The lower and higher quantiles don’t fit the line well.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-16-oktoberfest-analysis-part-ii_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We could again assume a normal distribution, but in my opinion we should try a non-parametric test here. We are going to use the Mann-Whitney-U-test to compare the groups. In R this test can be performed using the &lt;code&gt;wilcox.test()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wilcox.test(beer_cons_per_visitor ~ zlf, data = dt, conf.int = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Wilcoxon rank sum test
## 
## data:  beer_cons_per_visitor by zlf
## W = 123, p-value = 0.5643
## alternative hypothesis: true location shift is not equal to 0
## 95 percent confidence interval:
##  -0.1096337  0.1732736
## sample estimates:
## difference in location 
##             0.03441521&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The test result suggests that &lt;strong&gt;there is no statistical difference in beer consumption per visitor between ZLF and normal years&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;did-the-visitors-consume-more-hendl-in-zlf-years&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Did the Visitors Consume More Hendl in ZLF Years?&lt;/h3&gt;
&lt;p&gt;In the last part of our analysis of the ZLF and its influence we are going to look at the hendl consumption. The analysis is going to be performed the same way as above.&lt;/p&gt;
&lt;p&gt;Again, we start with a colored plot of the hendl consumption development and two box plots:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-16-oktoberfest-analysis-part-ii_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-16-oktoberfest-analysis-part-ii_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The data distribution looks not very similar for years with and without ZLF. The ZLF boxplot is a little bit more widespread than the other. Nevertheless, we are going to perform our tests as before:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for normal distribution of ZLF years
shapiro.test(dt[dt$zlf == 1,]$hendl_cons_per_visitor)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  dt[dt$zlf == 1, ]$hendl_cons_per_visitor
## W = 0.89192, p-value = 0.2088&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for normal distribution of normal years
shapiro.test(dt[dt$zlf == 0,]$hendl_cons_per_visitor)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  dt[dt$zlf == 0, ]$hendl_cons_per_visitor
## W = 0.87916, p-value = 0.007994&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-16-oktoberfest-analysis-part-ii_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This time the test for normal distribution suggests that at least the normal year data is not normally distributed. That means we definitely need a non parametric test to compare the group means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wilcox.test(hendl_cons_per_visitor ~ zlf, data = dt, conf.int = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Wilcoxon rank sum test
## 
## data:  hendl_cons_per_visitor by zlf
## W = 101, p-value = 0.7964
## alternative hypothesis: true location shift is not equal to 0
## 95 percent confidence interval:
##  -0.01828764  0.01335592
## sample estimates:
## difference in location 
##           -0.001082846&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Wilcoxon test confirms our impression that &lt;strong&gt;there is no statistical difference between the mean hendl consumption of both groups&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mean-daily-visitor-count-before-and-after-911&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mean Daily Visitor Count Before and After 9/11&lt;/h2&gt;
&lt;p&gt;Finally we will look at the hypothesis that &lt;em&gt;the mean daily visitor count decreased to a lower level after the 9/11 terror attacks&lt;/em&gt;. During the analysis above we saw a huge drop.&lt;/p&gt;
&lt;p&gt;The boxplot below shows the mean daily visitor distribution of both periods:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-16-oktoberfest-analysis-part-ii_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The boxplots show the distribution of mean daily visitor count before and after 9/11 varies. Let’s see what the comparison tests suggest:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for normal distribution of data before 2000
shapiro.test(dt[dt$year &amp;lt; 2000,]$visitors_day)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  dt[dt$year &amp;lt; 2000, ]$visitors_day
## W = 0.93676, p-value = 0.3434&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for normal distribution of data after 2000
shapiro.test(dt[dt$year &amp;gt;= 2000,]$visitors_day)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  dt[dt$year &amp;gt;= 2000, ]$visitors_day
## W = 0.96559, p-value = 0.7118&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- t.test(visitors_day ~ ifelse(year &amp;lt;= 2000, &amp;quot;After 9/11&amp;quot;, &amp;quot;Before 9/11&amp;quot;), 
              data = dt, conf.int = T)
res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  visitors_day by ifelse(year &amp;lt;= 2000, &amp;quot;After 9/11&amp;quot;, &amp;quot;Before 9/11&amp;quot;)
## t = 4.0736, df = 30.904, p-value = 0.0002989
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  15743.57 47322.60
## sample estimates:
##  mean in group After 9/11 mean in group Before 9/11 
##                  404062.5                  372529.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The t-test confirms our hypothesis. &lt;strong&gt;The estimated decrease in daily visitors after 9/11 is 31533 visitors&lt;/strong&gt;. The 95% confidence interval for the difference is 15743 to 47322 visitors. If we take the mean daily visitor count across the years before 9/11 that would mean a &lt;strong&gt;8 % decrease&lt;/strong&gt;. Nevertheless, keep in mind that the 9/11 terror attack could just be one reason. There are a lot of other possible reasons which have not been investigated in this analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In the second part of our analysis we showed that…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The years where the ZLF took place &lt;strong&gt;did not bring more visitors to the Oktoberfest, nor did it increase the beer or hendl consumption&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After the terror attacks in 9/11 the &lt;strong&gt;mean daily visitor count decreased by around 8 %&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgement&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Acknowledgement&lt;/h1&gt;
&lt;p&gt;For this part I would like to say thank you to the people who helped me with the ressources for that analysis.I would like to thank Frank Börger and the team from Munich Open Data for answering my questions on the data and providing additional resources. Another great thank you goes to my friend Pat forproofreading. Unfortuantely I could not find enough additional visitor data for the ZLF to provide further insights. Nevertheless, I want to thank Mrs Katharina Höninger (Agrarhistorische Bibliothek) and Mrs Christine Karrer (Bayerischer Bauernverband) for their support.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Oktoberfest Analysis Part I</title>
      <link>/post/oktoberfest-analysis-part-i/</link>
      <pubDate>Tue, 08 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/oktoberfest-analysis-part-i/</guid>
      <description>&lt;p&gt;Everybody knows the Oktoberfest which takes place in Munich every year. In this blog post series we are going to look into a public available data set and try to gain some insights about the Oktoberfest.&lt;/p&gt;
&lt;p&gt;In the first part we load and describe the data. Furthermore, we will analyse the price and consumption of beer and hendl (chicken) over the years.&lt;/p&gt;
&lt;p&gt;In the &lt;a href=&#34;https://somtom.github.io/post/oktoberfest-analysis-part-ii/&#34;&gt;second part&lt;/a&gt; we are going to have a closer look at the Bavarian Central Agricultural Festival - also known as “ZLF” - and its influence on beer- and hendl consumption, as well as on visitor count. Further, we are going to look at the influence of the 9/11 terror attacks.&lt;/p&gt;
&lt;div id=&#34;aim&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Aim&lt;/h1&gt;
&lt;p&gt;Since I am currently diving into the field of data analysis and machine learning, I decided to start my first public analysis in order to use the tools I have been learning so far. The aim of this exploratory data analysis was to create some insights about the Oktoberfest using the public available Oktoberfest data set from the &lt;a href=&#34;https://www.opengov-muenchen.de/&#34;&gt;Munich Open Data side&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Further, I wanted to explore the Munich Open Data API to export the data from the server. My biggest aim, though, is to improve my analytic skills by getting feedback from the community.&lt;br /&gt;
That is why I would really appreciate your feedback. Feel free to comment on this post and to contact me.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;methods-and-material&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Methods and Material&lt;/h1&gt;
&lt;p&gt;For the analysis I used the public available Oktoberfest data set which can be found &lt;a href=&#34;https://www.opengov-muenchen.de/dataset/oktoberfest/resource/e0f664cf-6dd9-4743-bd2b-81a8b18bd1d2&#34;&gt;here&lt;/a&gt;. Additional information which can not be found in the data description online, has been provided by the city of Munich via email contact.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-up-environment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Set up Environment&lt;/h1&gt;
&lt;p&gt;We will start this analysis by loading some required packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages({
require(httr, quietly = T)
require(tidyverse, quietly = T, warn.conflicts = F)
require(gridExtra, quietly = T, warn.conflicts = F)
require(grid, quietly = T, warn.conflicts = F)
})&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;importing-data-from-api&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Importing Data from API&lt;/h1&gt;
&lt;p&gt;Having the environment set up, we will use the &lt;em&gt;Munich Open Data API&lt;/em&gt; and the &lt;code&gt;httr&lt;/code&gt; library to load our data from the server, and store it as a &lt;code&gt;data.frame&lt;/code&gt; within R.&lt;br /&gt;
The resource-id (&lt;code&gt;e0f664cf-6dd9-4743-bd2b-81a8b18bd1d2&lt;/code&gt;), which we are going to use to get our data via the API, can be found under additional information (German: “zusätzliche Informationen”) at the bottom of the &lt;a href=&#34;https://www.opengov-muenchen.de/dataset/oktoberfest/resource/e0f664cf-6dd9-4743-bd2b-81a8b18bd1d2&#34;&gt;data set side&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;url &amp;lt;- &amp;quot;https://www.opengov-muenchen.de&amp;quot;
path &amp;lt;- &amp;quot;api/action/datastore_search?resource_id=e0f664cf-6dd9-4743-bd2b-81a8b18bd1d2&amp;quot;

# get raw results
raw.result &amp;lt;- GET(url = url, path = path)

# convert raw results to list
result &amp;lt;- content(raw.result)

# convert results to table
dt &amp;lt;- bind_rows(result$result$records)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a quick view at our data to see, if the import went well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(dt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 9
##   hendl_preis jahr  besucher_gesamt hendl~ dauer bier_~ bier_~ `_id` besu~
##   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;
## 1 4.77        1985  7.1             629520 16    54541  3.20       1 444  
## 2 3.92        1986  6.7             698137 16    53807  3.30       2 419  
## 3 3.98        1987  6.5             732859 16    51842  3.37       3 406  
## 4 4.19        1988  5.7             720139 16    50951  3.45       4 356  
## 5 4.22        1989  6.2             775674 16    51241  3.60       5 388  
## 6 4.47        1990  6.7             750947 16    54300  3.77       6 419&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes, the API data import worked!&lt;/p&gt;
&lt;p&gt;As we see, the columns have German names. We change this for all English speaking readers. By doing so, we also perform some class- and metric corrections.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dt &amp;lt;- dt %&amp;gt;% 
  transmute(id = `_id`,
            # class corrections
            year = as.integer(jahr),
            duration = as.integer(dauer),
            hendl_cons = as.integer(hendl_konsum),
            beer_price = as.numeric(bier_preis),
            hendl_price = as.numeric(hendl_preis),
            # class- and additional metric corrections
            beer_cons = as.integer(bier_konsum)*100, # change to L
            visitors_total = as.numeric(besucher_gesamt) * 1000000, # change to nr. of people
            visitors_day = as.integer(besucher_tag)*1000) # change to nr. of people&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-description&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Description&lt;/h1&gt;
&lt;p&gt;Our data have 9 columns and 33 rows. The data set contains yearly data on beer- and hendl (chicken) consumption from 1985 to 2017. It also provides information about the price of both as well as total visitors, mean daily visitors, and the duration of the Oktoberfest in each year.&lt;/p&gt;
&lt;p&gt;The table below gives a quick overview on variable names and their metrics.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;beer consumption&lt;/td&gt;
&lt;td&gt;L&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;beer price&lt;/td&gt;
&lt;td&gt;€/Liter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;hendl consumption&lt;/td&gt;
&lt;td&gt;Nr. of Chicken&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;hendl price&lt;/td&gt;
&lt;td&gt;€/half chicken&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;total visitors&lt;/td&gt;
&lt;td&gt;Nr.of People&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;daily visitors&lt;/td&gt;
&lt;td&gt;Nr. of People&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;data-munging&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Munging&lt;/h1&gt;
&lt;p&gt;Now we are going to start to get our hands dirty and work with the Oktoberfest data set. We add some information like the years when the Bavarian Central Agricultural Festival (ZLF) took place and other variables which we are going to use either in part 1 or 2 of our analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# generate vector with years in which zlf festival took place 
# (every three years up to 1996; every 4 years from 2000 on)
zlf_years &amp;lt;- c(seq(1810,1996, 3), seq(2000, max(dt$year),4))

dt &amp;lt;- dt %&amp;gt;% 
  mutate(zlf = factor(ifelse(year %in% zlf_years, 1, 0)),
         hendl_cons_per_visitor = hendl_cons / visitors_total,
         beer_cons_per_visitor = beer_cons / visitors_total) %&amp;gt;% 
  select(-id) # remove id column since we don&amp;#39;t use it in the analysis&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since there is no missing data in our data set, we can start our analysis.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;data-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Analysis&lt;/h1&gt;
&lt;p&gt;In our data analysis we are going to look at the following topics:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part I:&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Beer price and its consumption&lt;/li&gt;
&lt;li&gt;Hendl price and its consumption&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Part II:&lt;/strong&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Influence of the ZLF on some variables in our data&lt;/li&gt;
&lt;li&gt;Mean daily visitor count before and after 9/11&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We are going to use visualizations and simple modeling techniques to describe the data. Further, we are going to perform statistical tests to compare means in groups.&lt;/p&gt;
&lt;div id=&#34;beer-price-and-consumption&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beer Price and Consumption&lt;/h2&gt;
&lt;p&gt;Every year one of the most discussed topics around the Oktoberfest is the increased beer price. People are always complaining that the beer is too expensive. Since we have the historical data on beer prices, our first question on the data set is:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How did the beer price develope from 1985 to 2017?&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;modeling-beer-price-development-over-the-years&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Modeling Beer Price Development Over the Years&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Note: The currency used in the data is €. All prices before the year 2002 have been transformed from DM (German currency before 2002) to Euro.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-08-oktoberfest-analysis-part-i_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In fact, the price for 1L beer increased almost linear over the years. We will try to use a simple linear regression model to describe the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_beer &amp;lt;- lm(beer_price ~ year, data = dt)
summary(fit_beer)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = beer_price ~ year, data = dt)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.4185 -0.1280  0.0050  0.1286  0.5254 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -4.830e+02  8.603e+00  -56.15   &amp;lt;2e-16 ***
## year         2.447e-01  4.299e-03   56.92   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2352 on 31 degrees of freedom
## Multiple R-squared:  0.9905, Adjusted R-squared:  0.9902 
## F-statistic:  3240 on 1 and 31 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our model suggests that the beer price increased by &lt;strong&gt;0.24 Cents per year&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Fitting models to the years with different currencies separately, we see a small difference in estimated price increase per year:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_dm &amp;lt;- lm(beer_price ~ year, data = subset(dt, year &amp;lt; 2002))
summary(fit_dm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = beer_price ~ year, data = subset(dt, year &amp;lt; 2002))
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.236176 -0.112059 -0.009412  0.077647  0.260000 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -4.203e+02  1.399e+01  -30.04 8.17e-15 ***
## year         2.132e-01  7.022e-03   30.37 6.94e-15 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1418 on 15 degrees of freedom
## Multiple R-squared:  0.984,  Adjusted R-squared:  0.9829 
## F-statistic: 922.2 on 1 and 15 DF,  p-value: 6.944e-15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_euro &amp;lt;- lm(beer_price ~ year, data = subset(dt, year &amp;gt;= 2002))
summary(fit_euro)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = beer_price ~ year, data = subset(dt, year &amp;gt;= 2002))
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.206721 -0.013706  0.003794  0.020860  0.298456 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -5.817e+02  1.244e+01  -46.75   &amp;lt;2e-16 ***
## year         2.938e-01  6.192e-03   47.45   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1142 on 14 degrees of freedom
## Multiple R-squared:  0.9938, Adjusted R-squared:  0.9934 
## F-statistic:  2251 on 1 and 14 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In years with DM currency (1985 to 2001) the estimated yearly price increase was around &lt;strong&gt;0.21 Cents per year&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;In the years after the Euro has been introduced (&amp;gt;2001), the estimated yearly price increase was &lt;strong&gt;0.29 Cents per year&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-08-oktoberfest-analysis-part-i_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;influence-of-beer-price-on-consumption&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Influence of Beer Price on Consumption&lt;/h3&gt;
&lt;p&gt;Now that we confirmed that the price increased over the years, the next question which arises is:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Did this increase have a negative influence on mean beer consumption per visitor?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In order to answer this question we are going to start with a visualization of our data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-08-oktoberfest-analysis-part-i_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We do not see a decline in mean beer consumption per visitor over the years. In fact, since around year 1999 it seems that it even has been increasing steadily.&lt;/p&gt;
&lt;p&gt;To see how the data are correlated we perform a correlation test using the Pearson correlation coefficient:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor.test(dt$beer_cons_per_visitor, dt$beer_price)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s product-moment correlation
## 
## data:  dt$beer_cons_per_visitor and dt$beer_price
## t = 16.204, df = 31, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.8920491 0.9730963
## sample estimates:
##       cor 
## 0.9457298&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The test suggests a true correlation with a Pearson correlation coefficient of 0.95. The p-value is way below 0.05 and we have a narrow confidence interval. If we plot the data, all points seem to be around one line:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-08-oktoberfest-analysis-part-i_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This result would suggest that beer consumption increases the higher the beer price gets. Nevertheless, in my opinion, this is an example that &lt;strong&gt;correlation does not imply causation&lt;/strong&gt;. I think that the increased consumption is a separate phenomena and not caused by increased prices. One reason you can think of might be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Oktoberfest gets more and more popular outside of Munich. More and more people from all over the world go there in order to drink a “Maß” beer. For people coming from Australia, Sweden, etc. the beer is still cheap compared to their home. That is why they do not consume less beer with increasing prices since it is still cheap for them.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In conclusion the data set suggests that &lt;strong&gt;the price does not seem to have a negative influence on mean beer consumption per visitor&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hendl-price-and-consumption&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hendl Price and Consumption&lt;/h2&gt;
&lt;p&gt;On the Oktoberfest it is not just about drinking. A lot of people also like to have something to eat besides their beer. So what about the hendl consumption and price development?&lt;/p&gt;
&lt;div id=&#34;modeling-hendl-price-development&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Modeling Hendl Price Development&lt;/h3&gt;
&lt;p&gt;As before we are going to have a look at the price only first:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Important Notes&lt;/em&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;em&gt;The currency used in the data is €. All prices before the year 2002 have been transformed from DM (German currency before 2002) to Euro.&lt;/em&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Before the year 2000 the data are mean prices at the selling places. Since 2000 the data are only mean prices inside the tents.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-08-oktoberfest-analysis-part-i_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It seems that there was a huge price increase in year 2000. &lt;strong&gt;But we need to be careful&lt;/strong&gt;: As mentioned in the notes above, the data collection method changed in 2000. So we need to compare both periods separately. In the year 2013 we have some kind of price outlier. Here the price was raised a huge amount compared to the steady increase the years before. After that, the price dropped to a level which seems to lie on a line of steady price increase. We will try to model the price increase from year 2000 on using a linear regression model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_hendl &amp;lt;- lm(hendl_price ~ year, data = subset(dt, year &amp;gt;= 2000))
summary(fit_hendl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = hendl_price ~ year, data = subset(dt, year &amp;gt;= 2000))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.32939 -0.16118 -0.06931  0.10417  0.75069 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -426.73552   25.49174  -16.74 1.46e-11 ***
## year           0.21713    0.01269   17.11 1.05e-11 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2794 on 16 degrees of freedom
## Multiple R-squared:  0.9482, Adjusted R-squared:  0.9449 
## F-statistic: 292.7 on 1 and 16 DF,  p-value: 1.048e-11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model suggests that the hendl price increased by &lt;strong&gt;0.22 Cents per year&lt;/strong&gt; since 2000.&lt;/p&gt;
&lt;p&gt;When we add the model to our plot it looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-08-oktoberfest-analysis-part-i_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;influence-of-hendl-price-on-consumption&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Influence of Hendl Price on Consumption&lt;/h3&gt;
&lt;p&gt;Above we saw that the beer price did not have a negative influence on the beer consumption. We are going to ask the same question for the hendl price and consumption. To get a first overview, we plot the data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-08-oktoberfest-analysis-part-i_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the hendl consumption dropped in 2001 after slowly decreasing from 1991 on. After the drop, the consumption stayed at a constant level, whereas the price went up.&lt;br /&gt;
For me it looks like the hendl consumption being correlated with price until year 2000. After that the consumption reached a minimum bound. Maybe at this point price sensitivity of the remaining hendl consumers changes.&lt;/p&gt;
&lt;p&gt;We can check the correlation by performing a correlation test using the Pearson correlation coefficient:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;with(subset(dt, year &amp;lt; 2000),cor.test(hendl_cons_per_visitor*100, hendl_price))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s product-moment correlation
## 
## data:  hendl_cons_per_visitor * 100 and hendl_price
## t = -2.1586, df = 13, p-value = 0.05016
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.812214934 -0.001917789
## sample estimates:
##       cor 
## -0.513676&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;with(subset(dt, year &amp;gt;= 2000),cor.test(hendl_cons_per_visitor*100, hendl_price))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s product-moment correlation
## 
## data:  hendl_cons_per_visitor * 100 and hendl_price
## t = -0.98985, df = 16, p-value = 0.337
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.6357835  0.2552837
## sample estimates:
##        cor 
## -0.2402164&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The correlation coefficients suggest a negative correlation for both periods. Nevertheless, the tests confirm the null hypothesis, which says that there is &lt;strong&gt;no true correlation between the variables&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For the period before 2000 there is at least a strong tendency (&lt;code&gt;p = 0.0501576&lt;/code&gt;) for a true negative correlation. Despite that, the confidence interval for the correlation coefficient is very big, which gives us a strong uncertainty.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-08-oktoberfest-analysis-part-i_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In conclusion we can say that the &lt;strong&gt;price seems to have an influence on the hendl consumption up to a specific point&lt;/strong&gt;. Maybe we could explain the constant level of consumption after reaching that specific price margin with the following hypothesis:&lt;/p&gt;
&lt;p&gt;Normally people would buy a hendl up to their specific price boundary. That would mean a steady decrease in consumption with increasing price.&lt;/p&gt;
&lt;p&gt;Nevertheless, there are a lot of companies, which rent a table in a tent and invite their employees to come. Most of the time these companies give a free amount of hendl and beer consumption to their employees. I think that a big company’s price sensitivity concerning hendl and beer is &lt;strong&gt;not&lt;/strong&gt; as high as the price sensitivity of a normal Oktoberfest visitor. Thus, even with increasing prices the basic level of consumption does not change a lot.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In the first part of our quick analysis we showed that…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The beer price increased over the years. We modeled a increase of &lt;strong&gt;0.24 Cent per year&lt;/strong&gt; with differences before and after Euro introduction&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;increase in beer price did not have a negative influence on beer consumption&lt;/strong&gt;. The consumption even went up over the years&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hendl price also &lt;strong&gt;increased steadily&lt;/strong&gt;. For the years since 2000 our model estimated a price increase of &lt;strong&gt;0.22 Cent per year&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The hendl price actually seem to have a &lt;strong&gt;influence on hendl consumption up to a specific point&lt;/strong&gt;. We showed a tendency for negative correlation before year 2000.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, I learned that the Munich Open Data API is not that difficult to use.&lt;/p&gt;
&lt;p&gt;In the next part of our analysis we will have a closer look on the influence of the ZLF and the 9/11 terror attacks.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgement&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Acknowledgement&lt;/h1&gt;
&lt;p&gt;For this part I would like to say thank you to the people who helped me with the ressources for that analysis.I would like to thank Frank Börger and the team from Munich Open Data for answering my questions on the data and providing additional resources. Another great thank you goes to my friend Pat forproofreading.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example-talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0100</pubDate>
      
      <guid>/talk/example-talk/</guid>
      <description>&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://sourcethemes.com/academic/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>/project/deep-learning/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/deep-learning/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>/project/example-external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/example-external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Person Re-Identification System For Mobile Devices</title>
      <link>/publication/person-re-identification/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/publication/person-re-identification/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mobile visual clothing search</title>
      <link>/publication/clothing-search/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>/publication/clothing-search/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
